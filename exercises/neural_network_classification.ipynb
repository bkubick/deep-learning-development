{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70bdc70f",
   "metadata": {},
   "source": [
    "# Neural Network Classification Problem Fundamentals\n",
    "\n",
    "Classificaiton problems are used to \"classify\" inputs into a class as an output.\n",
    "\n",
    "**Binary Classification**\n",
    "* One example of classification problems is binary classification. Whether or not inputs map to be classified as something or not. For instance, is an email considered spam or not spam.\n",
    "\n",
    "**Multi-class Classificaiton**\n",
    "\n",
    "**Multi-label Classificaiton**\n",
    "\n",
    "\n",
    "## Topics\n",
    "1. Architecture of a neural network classfification model.\n",
    "2. Input shapes and output shapes of a classification model (features and labels).\n",
    "3. Creating custom data to view and fit.\n",
    "4. Steps in modeling.\n",
    "5. Differeneet classification evaluation methods.\n",
    "6. Saving and loading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea30efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dff24ac0",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\" Plots the decision boundary created by the model predicting X.\n",
    "    \"\"\"\n",
    "    # Grab the x and y limits of graph for the X values (with margin of 0.1)\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    \n",
    "    # Creating prediction data\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Create X value (we're going to make predictions on these)\n",
    "    x_in = np.c_[xx.ravel(), yy.ravel()]  # stack 2D arrays together\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(x_in)\n",
    "    \n",
    "    # Check for multi-class\n",
    "    if len(y_pred[0]) > 1:\n",
    "        # Multiclass classification\n",
    "        y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n",
    "    else:\n",
    "        # Binary classification\n",
    "        y_pred = np.round(y_pred).reshape(xx.shape)\n",
    "   \n",
    "    # Plot the decision boundary\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\" Creates the confusion matrix, and plots it \"\"\"\n",
    "    figsize = (10, 10)\n",
    "\n",
    "    # CReate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, tf.round(y_pred))\n",
    "\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize our confusion matrix\n",
    "    n_classes = cm.shape[0]\n",
    "\n",
    "    # Prettifying it\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    # Create a matrix plot\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Create classes\n",
    "    classes = False\n",
    "    if classes:\n",
    "        labels = classes\n",
    "    else:\n",
    "        labels = np.arange(cm.shape[0])\n",
    "\n",
    "    # Label the axes\n",
    "    ax.set(title='Confusion Matrix',\n",
    "          xlabel='Predicted Label',\n",
    "          ylabel='True Label',\n",
    "          xticks=np.arange(n_classes),\n",
    "          yticks=np.arange(n_classes),\n",
    "          xticklabels=labels,\n",
    "          yticklabels=labels)\n",
    "\n",
    "    # Make Labels bigger\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.xaxis.label.set_size(20)\n",
    "    ax.title.set_size(20)\n",
    "\n",
    "    # Set the threshold\n",
    "    threshold = (cm.max() + cm.min()) / 2\n",
    "\n",
    "    # Plot the text on each cell\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, f'{cm[i, j]} ({cm[i, j]:.1f}%)',\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i,j] > threshold else 'black',\n",
    "                size=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac8d19ea",
   "metadata": {},
   "source": [
    "## Example Data to Fit & view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa21a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 1000 examples\n",
    "n_samples = 1000\n",
    "\n",
    "# Making a dataset that sets 2 inputs (x and y position of a dot on a graph),\n",
    "# and 1 output (which circle the data lays on)\n",
    "X, y = make_circles(n_samples, noise=0.03, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the features\n",
    "X, y[:10]  # This is a binary classification problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fde80ad1",
   "metadata": {},
   "source": [
    "### Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "circles = pd.DataFrame({'X0': X[:, 0], 'X1': X[:, 1], 'label': y})\n",
    "circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeda379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ecb309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the input and output shapes\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at an example\n",
    "X[0], y[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0e739eb",
   "metadata": {},
   "source": [
    "## Modeling, Compiling, and Fitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e30571",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79985f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Model\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2,), name='InputLayer'),\n",
    "    tf.keras.layers.Dense(100, name='HiddenLayer-1'),\n",
    "    tf.keras.layers.Dense(1, name='OutputLayer')\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.legacy.SGD(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# 3. Fit Model\n",
    "model_1.fit(X, y, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluating Model\n",
    "model_1.evaluate(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "effc2681",
   "metadata": {},
   "source": [
    "### Improving Model\n",
    "\n",
    "The model above is hitting an accuracy of 50% which is just terrible!\n",
    "\n",
    "**NOTE**: the model above doesn't split the dataset into training and test."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3b3857d",
   "metadata": {},
   "source": [
    "#### Recreating Model to Visualize the Predictions & Optimize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving our model\n",
    "# 1. Create Model\n",
    "model_2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2,), name='Input'),\n",
    "    tf.keras.layers.Dense(100, name='Hidden-1'),\n",
    "    tf.keras.layers.Dense(10, name='Hidden-2'),\n",
    "    tf.keras.layers.Dense(1, name='Output')\n",
    "])\n",
    "\n",
    "# 2. Compiling Model\n",
    "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# 3. Fitting Model\n",
    "model_2.fit(X, y, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b54c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate Model\n",
    "model_2.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc5f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the decision boundary of model_2\n",
    "plot_decision_boundary(model_2, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b18143a",
   "metadata": {},
   "source": [
    "#### Analyzing plot\n",
    "\n",
    "Well there's your problem. Our model is trying to treat the binary output as a linear function. This is suggesting that we forgot to introduce non-linearity into our model!\n",
    "\n",
    "**Non-Linearity**\n",
    "Non-linearity in Neural Networks is introduced through the activation functions for each hidden layer. Without defining the activation functions, there is no non-linearity introduced into the model, so we cannot generate outputs of anything other than linear plots."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b307b6c8",
   "metadata": {},
   "source": [
    "### Improving Model by Introducing Non-Linearity (Activation Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the model (using a classification activation function (sigmoid))\n",
    "model_3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2,), name='Input'),\n",
    "    tf.keras.layers.Dense(5, activation=tf.keras.activations.relu, name='Hidden-1'),  # Relu introduces non-linearity\n",
    "    tf.keras.layers.Dense(5, activation=tf.keras.activations.relu, name='Hidden-2'),  # Relu introduces non-linearity\n",
    "    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid, name='Output')  # Sigmoid outputs a 1 or 0, so good for binary classification\n",
    "])\n",
    "\n",
    "# 2. Compiling the model\n",
    "model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# 3. Fitting the model\n",
    "history_3 = model_3.fit(X, y, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e73e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluating the model\n",
    "model_3.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c12802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the decision boundary of model_3\n",
    "plot_decision_boundary(model_3, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7de13cf3",
   "metadata": {},
   "source": [
    "### Evaluating & Improving Our Model\n",
    "\n",
    "First step is to do this right and setup a training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb69e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data using Sk learn\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12229bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "X_train[:5], y_train[:5], len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c33a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Model\n",
    "model_4 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2,), name='Input'),\n",
    "    tf.keras.layers.Dense(5, activation=tf.keras.activations.relu, name='Hidden-1'),\n",
    "    tf.keras.layers.Dense(5, activation=tf.keras.activations.relu, name='Hidden-2'),\n",
    "    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid, name='Output')\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "model_4.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.01),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# 3. Fit Model\n",
    "history_4 = model_4.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd12fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_4.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b71544",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Train')\n",
    "plot_decision_boundary(model_4, X_train, y_train)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Test')\n",
    "plot_decision_boundary(model_4, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aeaf4e87",
   "metadata": {},
   "source": [
    "### Visualizing the Training of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the history data during the fit?\n",
    "history_4.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "history_4_df = pd.DataFrame(history_4.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de4768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data\n",
    "history_4_df.plot()\n",
    "plt.title('Model 4 Loss Curve')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f740939e",
   "metadata": {},
   "source": [
    "## Finding Best Learning Rate\n",
    "#### Using Loss Curves (See Above) to Determine Best Learning Rate\n",
    "\n",
    "To find the ideal learning rate (the learnig rate where the loss decreeases the most during training) we're going to use the following steps\n",
    "1. A learning rate callback: An extra piece of functionality you can add to your *while* its training.\n",
    "2. Another model (we could use the same one as above, but we're practicing building models here)\n",
    "3. A modified loss curves plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new model to find best learning rate using callback\n",
    "\n",
    "# 1. Create Model\n",
    "model_5 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2,)),\n",
    "    tf.keras.layers.Dense(5, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(5, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "model_5.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# 2.1. Creating learning rate callback\n",
    "learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20))\n",
    "\n",
    "# 3. Fit the model with the learning rate scheduler\n",
    "history_5 = model_5.fit(X_train, y_train, epochs=100, callbacks=[learning_rate_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43508799",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_5_df = pd.DataFrame(history_5.history)\n",
    "history_5_df.plot()\n",
    "plt.title('Model 5 (Learning Rate Scheduler)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning rate versus the loss\n",
    "lrs = 1e-4 * (10 ** (tf.range(100) / 20))\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.semilogx(lrs, history_5.history['loss'])\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Rate vs Loss')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b274fdce",
   "metadata": {},
   "source": [
    "#### Finding Ideal Learning Rate from Above Graph\n",
    "The ideal learning rate is between slightly before where the learning rate \"flattens out\", and the lowest point on the curve.\n",
    "\n",
    "For the above example, the ideal learning rate is somewhere betwee, .01 and .1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51cf51f2",
   "metadata": {},
   "source": [
    "### Try a New Model using Learning Rate with .03 to See if that Improves Model-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391f4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Model\n",
    "model_6 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2,)),\n",
    "    tf.keras.layers.Dense(5, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(5, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)\n",
    "])\n",
    "\n",
    "# 2. Compile Model with the Ideal Learning Rate found in Plot Above (0.03)\n",
    "model_6.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.03),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# 3. Fit Model\n",
    "model_6.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddfe5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_6 = model_6.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfeb7d08",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "The epoch doesn't hit 99% accuracy for model_4 until ~13 epochs, where model_5 hits 99% accuracy at ~9 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f9e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Train')\n",
    "plot_decision_boundary(model_6, X_train, y_train)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Test')\n",
    "plot_decision_boundary(model_6, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "194ba61d",
   "metadata": {},
   "source": [
    "## More Classificaiton Evaluation Methods\n",
    "\n",
    "Alongside visualizing our model results as mush as possible, kthere are a handful of other classification evaluation methods & metrics to be familiar with:\n",
    "* Accuracy - Most Common\n",
    "* Precision - Less false positives\n",
    "* Recall - Less false negatives\n",
    "* F1-score\n",
    "* Confusion Matrix\n",
    "* SKLearn Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the accuracy of our model\n",
    "loss, accuracy = model_6.evaluate(X_test, y_test)\n",
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb20fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix of our Model\n",
    "# Note: y pred comes out as a decimal for its estimates, so to get actual guess in binary form, need to round the prediction.\n",
    "confusion_matrix(y_test, tf.round(y_pred_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred_6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
