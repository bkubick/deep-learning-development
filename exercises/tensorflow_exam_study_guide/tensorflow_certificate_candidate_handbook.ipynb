{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24747fd8-5301-4c67-9aaf-5021fa0a3370",
   "metadata": {},
   "source": [
    "# TensorFlow Developer Exam Overview\n",
    "\n",
    "The purpose of this notebook is a deep dive into preparing for the TensorFlow Developer Certificate exam. This includes very useful information regarding building and tuning deep learning models, along with a full overview on every section listed in the [TensorFlow Certificate Candidate Handbook](https://www.tensorflow.org/static/extras/cert/TF_Certificate_Candidate_Handbook.pdf).\n",
    "\n",
    "In addition to specific information that is useful to know when building models, there is also a list of resources & readings where I grab this information from which go deeper into the understanding/details of specific information.\n",
    "\n",
    "I have listed out external and personal notebooks, github repos, and datasets that would be useful to go over or reference as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ff721-70dc-4bc1-9b5a-18b5a0a00de0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Notebook Information\n",
    "\n",
    "This section goes over the details, information, Table of Contents, and a list of resources referenced in this notebook and readings that are useful to know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e998b-cebf-4934-94d0-11e92f1f9bee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports\n",
    "\n",
    "All the packages required to fully run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b5f97fb-b2bf-4b11-a2bc-0fc8a8248033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d76ec8-28d7-48b0-80dc-dc81ff50a35e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Details\n",
    "\n",
    "Information pertaining to this specific notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041f1da7-4228-4570-bd77-8282c1579de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook last run (end-to-end): 2023-10-03 07:48:55.497323\n"
     ]
    }
   ],
   "source": [
    "print(f'Notebook last run (end-to-end): {datetime.datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2de765-066a-4d8f-a5b0-84300977bbbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Table of Contents\n",
    "\n",
    "The various sections and links to those section in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49600a-e015-434a-a05c-a72be4480f59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Section-1: Guide to TensorFlow & Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5110f-7789-42ca-ad57-1a1b21b81f75",
   "metadata": {},
   "source": [
    "1. TensorFlow Inputs & Outputs Table\n",
    "   * 1.1. Inputs\n",
    "   * 1.2. Outputs\n",
    "2. Overfitting\n",
    "3. Pooling\n",
    "   * 3.1. Average Pooling\n",
    "   * 3.2. Max Pooling\n",
    "   * 3.3. Global Pooling\n",
    "4. Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b1d9b-3225-4d67-b622-c401c218275a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Section-2 TensorFlow Certificate Candidate Handbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83eb652-2dd3-41b2-9840-31ee3d6a25bf",
   "metadata": {},
   "source": [
    "1. TensorFlow Development Skills\n",
    "    - 1.1. Know how to program in Python, resolve Python issues, and compile and run Python programs in PyCharm.\n",
    "    - 1.2. Know how to find information about TensorFlow APIs, including how to find guides and API references on tensorflow.org.\n",
    "    - 1.3. Know how to debug, investigate, and solve error messages from the TensorFlow API.\n",
    "    - 1.4. Know how to search beyond tensorflow.org, as and when necessary, to solve your TensorFlow questions.\n",
    "    - 1.5. Know how to create ML models using TensorFlow where the model size is reasonable for the problem being solved.\n",
    "    - 1.6. Know how to save ML models and check the model file size.\n",
    "    - 1.7. Understand the compatibility discrepancies between different versions of TensorFlow.\n",
    "2. Building and training neural network models using TensorFlow 2.x\n",
    "    - 2.1. Use TensorFlow 2.x.\n",
    "    - 2.2. Build, compile and train machine learning (ML) models using TensorFlow.\n",
    "    - 2.3. Preprocess data to get it ready for use in a model.\n",
    "    - 2.4. Use models to predict results.\n",
    "    - 2.5. Build sequential models with multiple layers.\n",
    "    - 2.6. Build and train models for binary classification.\n",
    "    - 2.7. Build and train models for multi-class categorization.\n",
    "    - 2.8. Plot loss and accuracy of a trained model.\n",
    "    - 2.9. Identify strategies to prevent overfitting, including augmentation and dropout.\n",
    "    - 2.10. Use pretrained models (transfer learning).\n",
    "    - 2.11. Extract features from pre-trained models.\n",
    "    - 2.12. Ensure that inputs to a model are in the correct shape.\n",
    "    - 2.13. Ensure that you can match test data to the input shape of a neural network.\n",
    "    - 2.14. Ensure you can match output data of a neural network to specified input shape for test data.\n",
    "    - 2.15. Understand batch loading of data.\n",
    "    - 2.16. Use callbacks to trigger the end of training cycles.\n",
    "    - 2.17. Use datasets from different sources.\n",
    "    - 2.18. Use datasets in different formats, including json and csv.\n",
    "    - 2.19. Use datasets from tf.data.datasets.\n",
    "3. Image Classification\n",
    "    - 3.1. Define Convolutional neural networks with Conv2D and pooling layers.\n",
    "    - 3.2. Build and train models to process real-world image datasets.\n",
    "    - 3.3. Understand how to use convolutions to improve your neural network.\n",
    "    - 3.4. Use real-world images in different shapes and sizes.\n",
    "    - 3.5. Use image augmentation to prevent overfitting.\n",
    "    - 3.6. Use ImageDataGenerator.\n",
    "    - 3.7. Understand how ImageDataGenerator labels images based on the directory structure.\n",
    "4. Natural language processing (NLP)\n",
    "    - 4.1. Build natural language processing systems using TensorFlow.\n",
    "    - 4.2. Prepare text to use in TensorFlow models.\n",
    "    - 4.3. Build models that identify the category of a piece of text using binary categorization\n",
    "    - 4.4. Build models that identify the category of a piece of text using multi-class categorization\n",
    "    - 4.5. Use word embeddings in your TensorFlow model.\n",
    "    - 4.6. Use LSTMs in your model to classify text for either binary or multi-class categorization.\n",
    "    - 4.7. Add RNN and GRU layers to your model.\n",
    "    - 4.8. Use RNNS, LSTMs, GRUs and CNNs in models that work with text.\n",
    "    - 4.9. Train LSTMs on existing text to generate text (such as songs and poetry)\n",
    "5. Time series, sequences and predictions\n",
    "    - 5.1. Train, tune and use time series, sequence and prediction models.\n",
    "    - 5.2. Train models to predict values for both univariate and multivariate time series.\n",
    "    - 5.3. Prepare data for time series learning.\n",
    "    - 5.4. Understand Mean Absolute Error (MAE) and how it can be used to evaluate accuracy of sequence models.\n",
    "    - 5.5. Use RNNs and CNNs for time series, sequence and forecasting models.\n",
    "    - 5.6. Identify when to use trailing versus centred windows.\n",
    "    - 5.7. Use TensorFlow for forecasting.\n",
    "    - 5.8. Prepare features and labels.\n",
    "    - 5.9. Identify and compensate for sequence bias.\n",
    "    - 5.10. Adjust the learning rate dynamically in time series, sequence and prediction models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b4cb4-9967-4987-a083-cd8b6d7ca3e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Resources/Readings\n",
    "\n",
    "The various resources/articles used to create this notebook, along with various resources/articles that are very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ae5fd-9cf7-4088-ae25-ddb334097fed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Repositories\n",
    "1. [mrdbourke TensorFlow Course Repository](https://github.com/mrdbourke/tensorflow-deep-learning/tree/main)\n",
    "2. [kolasniwash Tensorflow Certificate Study Guide](https://github.com/kolasniwash/tensorflow-certification-study-guide)\n",
    "\n",
    "#### Test Taker Articles\n",
    "3. [Article by Judy Traj](https://medium.com/@judytraj007/getting-the-google-tensorflow-developer-certification-51cf1e4c2bf9)\n",
    "4. [Article by R. Barbero](https://medium.com/@rbarbero/tensorflow-certification-tips-d1e0385668c8)\n",
    "5. [LinkedIn Article by Vivek Bomabtkar](https://www.linkedin.com/pulse/tensorflow-developer-certification-vivek-bombatkar/)\n",
    "\n",
    "#### Tutorials/Guides\n",
    "6. [TensorFlow: Guide to Getting Started](https://www.kaggle.com/code/nicholasjhana/tensorflow-guide-to-getting-started/notebook)\n",
    "\n",
    "#### Notebooks\n",
    "7. [LLM Notebook in TensorFlow by FOUCARDM on Kaggle](https://www.kaggle.com/code/foucardm/tensorflow-certification-guide-text-data/notebook)\n",
    "8. [Univariate Time Series Forecasting with TensorFlow](https://www.kaggle.com/code/nicholasjhana/univariate-time-series-forecasting-with-keras/notebook)\n",
    "9. [Multi-Variate Time Series Forecasting with TensorFlow](https://www.kaggle.com/code/nicholasjhana/multi-variate-time-series-forecasting-tensorflow/notebook)\n",
    "\n",
    "#### Documentation\n",
    "10. [Bidirectional RNN Architecture](https://www.geeksforgeeks.org/bidirectional-lstm-in-nlp/)\n",
    "11. [LSTM Architecture](https://www.geeksforgeeks.org/deep-learning-introduction-to-long-short-term-memory/)\n",
    "12. [GRU Architecture](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)\n",
    "13. [Metrics in Timeseries Forecasting](https://mlpills.dev/time-series/error-metrics-for-time-series-forecasting/)\n",
    "14. [Reducing Overfitting](https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html)\n",
    "15. [Data Augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation)\n",
    "16. [Layer Regularizers](https://johnthas.medium.com/regularization-in-tensorflow-using-keras-api-48aba746ae21)\n",
    "17. [Global Average Pooling](https://iq.opengenus.org/global-average-pooling/)\n",
    "18. [Introduction to Pooling Layers in CNN](https://towardsai.net/p/l/introduction-to-pooling-layers-in-cnn)\n",
    "19. [Global Pooling in Convolutional Neural Networks](https://blog.paperspace.com/global-pooling-in-convolutional-neural-networks/)\n",
    "20. [A Guide to Convolution Arithmetic for Deep Learning](https://arxiv.org/pdf/1603.07285v1.pdf)\n",
    "21. [Understand Transposed Convolutions and Build your own Transposed Convolution Layer from Scratch](https://towardsdatascience.com/understand-transposed-convolutions-and-build-your-own-transposed-convolution-layer-from-scratch-4f5d97b2967)\n",
    "22. [Improving Performance of Convolutional Neural Network](https://medium.com/@dipti.rohan.pawar/improving-performance-of-convolutional-neural-network-2ecfe0207de7)\n",
    "23. [Activation Functions](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)\n",
    "24. [Univariate Time Series](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc44.htm)\n",
    "25. [Multivariate Time Series](https://towardsdatascience.com/a-step-by-step-guide-to-feature-engineering-for-multivariate-time-series-162ccf232e2f)\n",
    "26. [Single vs. Multivariate Time Series](https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/#Univariate_Vs._Multivariate_Time_Series_Forecasting_Python)\n",
    "27. [Trailing vs. Centered Moving Average](https://machinelearningmastery.com/moving-average-smoothing-for-time-series-forecasting-python/)\n",
    "28. [Learning Curves for Diagnosing Machine Learning Performance](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)\n",
    "\n",
    "#### Tools\n",
    "29. [CNN Explainer](https://poloclub.github.io/cnn-explainer/)\n",
    "30. [NN Playground](https://playground.tensorflow.org/)\n",
    "\n",
    "#### Study Guides\n",
    "31. [TesnsorFlow Exam Study Guide (In spanish)](https://tensorflow.backprop.fr/build-and-train-neural-network-models-using-tensorflow-2-x/use-tensorflow-2-x/)\n",
    "32. [Zero to Mastery - Book Version of Course](https://dev.mrdbourke.com/tensorflow-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f965c-d91e-4f60-84e7-a0cce3a14b7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Section-1: Guide to TensorFlow & Modeling\n",
    "\n",
    "TensorFlow and Modeling has alottt of information, but there are certain key areas that are extremely useful to keep in mind. This section will go over the information that is absolutely crucial in creating good models with TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4f44e4-e36b-4f66-b42e-3e6d4ba23a82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### [1. TensorFlow Inputs & Outputs Table](https://www.kaggle.com/code/nicholasjhana/tensorflow-guide-to-getting-started/notebook#Inputs)\n",
    "\n",
    "When building models, there are certain things to remember depending on the specific problem being modeled. The following tables summarize the input/output and model configurations for each type of the various problems being solved [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce808ce-189c-433d-a4e6-900a82e1d142",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Inputs\n",
    "\n",
    "Input shapes depend on the type of problem and network architecture. Input shape can be defined in the first layer of the network either calling the `input_shape` parameter or using the `tf.keras.layers.Input` class.\n",
    "\n",
    "| Data Type      | Input Shape                                      |\n",
    "| :--------------| :------------------------------------------------|\n",
    "| Image          | (image height, image width, number of channels)  |\n",
    "| Sequence       | (number of sequence steps, number of features)   |\n",
    "| Structured     | (samples/batch size, features)                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93325e51-4c98-4b84-b538-43d395ffed90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Outputs\n",
    " \n",
    "| Problem Type\t        | Output Neurons        | Target Format   |\tLoss Type\t                               | Last Neuron Activation |\n",
    "| :---------------------| :-------------------- | :-------------- | :----------------------------------------- | :--------------------- |\n",
    "| Binary Classification\t| 1\t                    | Binary\t      | binary_crossentropy                        | sigmoid                |\n",
    "| Multi Classification\t| Number of classes  \t| One-Hot Encoded | categorical_crossentropy                   | softmax                |\n",
    "| Multi Classification\t| Number of classes\t    | Label Encoded\t  | sparse_categorical_crossentropy\t           | softmax                |\n",
    "| Regression\t        | Number of predictions\t| Numeric\t      | Any regression metric: MSE/RMSE/MSLE/Huber | None                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408169b-e5fa-49ae-b888-87344d741859",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### [2. Overfitting](https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html)\n",
    "\n",
    "Overfitting occurs when the model's predictions become highly variant. That is we see large variations between predictions in an effort to fit closer to the training set. The opposite can also occur, underfitting where the predictions do not generalize effectively.\n",
    "\n",
    "There are a few methods to recognize if the model overfit the training data.\n",
    "\n",
    "* Training loss declines while validation loss is constant or rises.\n",
    "* A large gap between the training accuracy/ROC AUC/etc and the validation.\n",
    "* Validation score does not change while validation loss declines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c451d70-2aaf-44f0-8ac1-74024f4a0107",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### [3. Pooling](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/)\n",
    "\n",
    "Pooling layers provide an approach to down sampling feature maps by summarizing the presence of features in patches of the feature map. Pooling involves selecting a pooling operation, much like a filter to be applied to feature maps. The size of the pooling operation or filter is smaller than the size of the feature map; specifically, it is almost always 2×2 pixels applied with a stride of 2 pixels.\n",
    "\n",
    "This means that the pooling layer will always reduce the size of each feature map by a factor of 2, e.g. each dimension is halved, reducing the number of pixels or values in each feature map to one quarter the size. For example, a pooling layer applied to a feature map of 6×6 (36 pixels) will result in an output pooled feature map of 3×3 (9 pixels).\n",
    "\n",
    "The pooling operation is specified, rather than learned. Two common functions used in the pooling operation are:\n",
    "\n",
    "* **Average Pooling**: Calculate the average value for each patch on the feature map.\n",
    "* **Maximum Pooling (or Max Pooling)**: Calculate the maximum value for each patch of the feature map.\n",
    "\n",
    "In addition to Max and Average pooling, we can also do Global or Normal pooling for both Max and Average pooling ([Global Average Pooling](https://iq.opengenus.org/global-average-pooling/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bfefe5-1bf3-4259-a274-ae85e19c01c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Avergage Pooling\n",
    "\n",
    "One of the types of pooling that isn’t used very often is average pooling, instead of taking the max within each filter we take the average. It does the same task as max pooling which is to reduce the dimensionality of images. This results in avergaging out the features rather than making prominant features more prominant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332c542-6556-4fdd-b3a6-5e71707fc1c4",
   "metadata": {},
   "source": [
    "<img src=\"screenshots/average_pooling.png\" alt=\"Average Pooling\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4a655-1969-455e-bd8a-85aa202b5b74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Max Pooling\n",
    "\n",
    "Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map.\n",
    "\n",
    "The results are down sampled or pooled feature maps that highlight the most present feature in the patch, not the average presence of the feature in the case of average pooling. This has been found to work better in practice than average pooling for computer vision tasks like image classification.\n",
    "\n",
    "&#128273; **NOTE** This is commonly used with computer vision because it simply reduces images to smaller feature maps, highlighting prominant features within the top level image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6a5d47-3a07-4837-aaaf-db7420c402e6",
   "metadata": {},
   "source": [
    "<img src=\"screenshots/max_pooling.png\" alt=\"Average Pooling\" style=\"width:600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a8123-ca27-409d-903c-4d2f005b38e7",
   "metadata": {},
   "source": [
    "#### Global Pooling\n",
    "\n",
    "The pooling technique reduces each feature map channel to a single value. This value depends on the type of global pooling, which can be any of the previously explained pooling types. In other words, applying global pooling is similar to using a filter of the exact dimensions of the feature map.\n",
    "\n",
    "Global pooling layers often replace the Flatten or Dense output layers.\n",
    "\n",
    "&#128273; **NOTE** This is commonly used with language models to reduce the multi-dimensional embeddings into a single, flattened out feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214f107-0772-4602-8dae-909844cf4c26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Section-2: TensorFlow Certificate Candidate Handbook\n",
    "\n",
    "The TensorFlow certificate exam provides a handbook that gives details on what to know for the exam. This section goes over each section of the Handbook, providing sample information regarding each question.\n",
    "\n",
    "* [TensorFlow Certificate Candidate Handbook](https://www.tensorflow.org/static/extras/cert/TF_Certificate_Candidate_Handbook.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c333a-4cf4-4ee1-9bf8-c3f25d397638",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. TensorFlow Development Skills\n",
    "You need to demonstrate that you understand how to develop software programs using TensorFlow and that you can find the information you need to work as an ML practitioner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de92e3c0-c5d2-466d-8192-17cd1147eb04",
   "metadata": {},
   "source": [
    "#### 1.1. Know how to program in Python, resolve Python issues, and compile and run Python programs in PyCharm.\n",
    "* [Downloading PyCharm](https://www.jetbrains.com/pycharm/)\n",
    "* [Learn PyCharm](https://www.jetbrains.com/pycharm/learn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b7db3-face-42d7-af9c-1fe737a56791",
   "metadata": {},
   "source": [
    "#### 1.2. Know how to find information about TensorFlow APIs, including how to find guides and API references on tensorflow.org.\n",
    "* [API Documentation](https://www.tensorflow.org/api_docs/python/tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287958a3-1b41-4b68-b7ad-307b5f5eb108",
   "metadata": {},
   "source": [
    "#### 1.3. Know how to debug, investigate, and solve error messages from the TensorFlow API.\n",
    "* [Debugging Tips](https://towardsdatascience.com/debugging-in-tensorflow-392b193d0b8)\n",
    "* [TensorFlow Errors](https://www.tensorflow.org/api_docs/python/tf/errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e856802-1d2d-4678-89d1-4147237a760f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1.4. Know how to search beyond tensorflow.org, as and when necessary, to solve your TensorFlow questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33ecfe2-a626-4a01-a883-d69df8ae2400",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1.5. Know how to create ML models using TensorFlow where the model size is reasonable for the problem being solved.\n",
    "\n",
    "* Sequential Model\n",
    "* Functional API Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82213540-2f7d-41bc-b5c8-a1e7959d65ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Sequential Model\n",
    "\n",
    "* `tf.keras.models.Sequential`\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "032fc5c1-dfad-4cfe-93e3-166502bdec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2,), name='input_layer'),\n",
    "    tf.keras.layers.Dense(10, activation='relu', name='hidden_layer_1'),\n",
    "    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid, name='output'),\n",
    "], name='sequential_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d9c282-c764-45e2-94f0-256aa07787aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Functional API\n",
    "\n",
    "* `tf.keras.models.Model`\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc49b74-b299-4106-a7b8-7b0c1de7b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(2,), name='input_layer')\n",
    "x = tf.keras.layers.Dense(10, activation='relu', name='hidden_layer_1')(inputs)\n",
    "outputs = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid, name='output')(x)\n",
    "\n",
    "functional_model = tf.keras.models.Model(inputs, outputs, name='functional_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f826f3d0-ca77-4860-9a8e-8c259d9ce89c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1.6. Know how to save ML models and check the model file size.\n",
    "\n",
    "Models can be saved in one of two formats: `h5` or `TF`.\n",
    "\n",
    "* [Saving and Loading Models Article](https://www.kdnuggets.com/2021/02/saving-loading-models-tensorflow.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f3785c-a274-4b9a-91d8-a1fbc40d08ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "332d3a8b-2854-4017-a956-308284cf3132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./models/sequential_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/sequential_model/assets\n"
     ]
    }
   ],
   "source": [
    "sequential_model.save('./models/sequential_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9470a43-3b39-4d5f-b28e-d54bfe638acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonkubick/Code/deep-learning-development/env/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "functional_model.save('./models/functional_model', save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad1dc5-3602-473f-be57-01b8a8aa0a0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fe22232-4bc5-451f-a4c7-9d1dcf6a2c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "sequential_model_loaded = tf.keras.models.load_model('./models/sequential_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54b143ef-bc8c-4379-802d-01d0dbf24b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "functional_model_loaded = tf.keras.models.load_model('./models/functional_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32776ea-9866-4efa-b40a-6cf37218b323",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Checking File Size\n",
    "\n",
    "**TODO** Come back to check this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "533ba01b-ceef-41e1-aecc-1609876fb51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50965"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_size = 0\n",
    "for dir, _, files in os.walk('./models/sequential_model'):\n",
    "    total_size += sum([os.path.getsize(f'{dir}/{file}') for file in files])\n",
    "\n",
    "total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a79851e-843e-4e15-adcc-21367d39c79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16016"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.getsize('./models/functional_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ef76b4-9cc6-4f2f-bb74-74468e3adc5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1.7. Understand the compatibility discrepancies between different versions of TensorFlow.\n",
    "\n",
    "* https://www.tensorflow.org/guide/migrate/tf1_vs_tf2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf7e62-0a12-4000-b9d9-b1f26bceb5bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. Building and training neural network models using TensorFlow 2.x\n",
    "\n",
    "You need to understand the foundational principles of machine learning (ML) and deep learning (DL) using TensorFlow 2.x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8e7c3-e662-4e8d-92a1-9062fa5d1064",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.1. Use TensorFlow 2.x.\n",
    "\n",
    "* https://blog.tensorflow.org/2019/09/tensorflow-20-is-now-available.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d9d344-2316-459e-877c-8078ebdbcd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d50afc-51be-42ee-a007-4fb51491ead2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.2. Build, compile and train machine learning (ML) models using TensorFlow.\n",
    "\n",
    "**Building Models**\n",
    "* `tf.keras.models.Sequential`: Simple way of creating models with a single input, and are sequentially setup layers.\n",
    "* `tf.keras.models.Model`: More customizable way of creating models, allowing multiple inputs, parallel/wide networks, and deep networks.\n",
    "\n",
    "**Compiling Models**\n",
    "* `model.compile`: Used to set how the model should learn through setting the optimizer and loss function.\n",
    "\n",
    "**Training Models**\n",
    "* `model.fit`: Used to actually train the models through passing in all the training specific information (data, epochs, validation data, batch size, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f79350e8-129d-4edd-8f2c-fc607a6b0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model (Sequential API)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(5),\n",
    "])\n",
    "\n",
    "# Build Model (Functional API)\n",
    "inputs = tf.keras.layers.Input(shape=(1,))\n",
    "outputs = tf.keras.layers.Dense(5)(inputs)\n",
    "model = tf.keras.models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f935504-5174-4392-900c-891f7070f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model.compile(loss='mae',\n",
    "              optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "              metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b96943e-e7c5-45ea-b854-c26490b35999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "history = model.fit(\n",
    "    x=[1],\n",
    "    y=[[1,2, 3, 4, 5]],\n",
    "    epochs=1,\n",
    "    callbacks=[],\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec02ea-f892-46a2-aba8-a710391d5310",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.3. Preprocess data to get it ready for use in a model.\n",
    "\n",
    "* Categorical Data needs to be converted to a numerical representation\n",
    "    - Encode data using `sklearn.preprocessing.LabelEncoder`\n",
    "    - One-hot Encode data using `sklearn.preprocessing.OneHotEncoder` or `tf.one_hot`\n",
    "* Preprocessing Layers\n",
    "    - [Preprocessing Layers](https://www.tensorflow.org/guide/keras/preprocessing_layers)\n",
    "* Normalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf905d5-3aeb-40af-94f2-ca5734c107b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.4. Use models to predict results.\n",
    "\n",
    "* `model.predict`: Used to predict the results given the corresponding inputs.\n",
    "\n",
    "**NOTE**: the prediction outputs the probabilities in classification models, or the value in regression models. This is problematic for classification models, because classification means that is should be one value of a set of options rather than a probability.\n",
    "\n",
    "* `tf.argmax`: Grabs the index of the maximum value (used to grab the corresponding classification class).\n",
    "* `tf.round`: Rounds the probabilities. This doesn't always work for multi-class classification because all the probs can round to 0 if there are many classifications it can fall into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27ca230e-b638-46a4-bdb7-6975b89a74ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=2>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax([0.1, 0.2, 0.6, 0.12, 0.14])  # Highest index is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d950ff38-2be0-4053-bce4-e35a4dd5899b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([0., 1., 0., 1., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.round([0.1, 0.51, 0.3, 0.6, 0.12, 0.14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaeaac4-7a56-4e7e-a23a-b4c0f2f138b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.5. Build sequential models with multiple layers.\n",
    "\n",
    "* `tf.keras.models.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaf9680b-899f-468f-9f83-b8c66e343f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,), name='Input'),\n",
    "    tf.keras.layers.Dense(5),\n",
    "    tf.keras.layers.Dense(5),\n",
    "    tf.keras.layers.Dense(5),\n",
    "    tf.keras.layers.Dense(1, name='Output'),\n",
    "], name='SequentialModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b8dbe-6676-4094-bdc3-09392c966a5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.6. Build and train models for binary classification.\n",
    "\n",
    "Binary classification is when the output is either 1 or 0 (only two choices).\n",
    "\n",
    "* Ouput Shape = `(1,)`\n",
    "* Output Activation Function = `sigmoid`\n",
    "* Loss Function = `binary_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74d1af02-746e-4d45-9702-36cb710facfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Model\n",
    "binary_classification_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "], name='binary_classification')\n",
    "\n",
    "# Binary Model Compiling\n",
    "binary_classification_model.compile(loss='binary_crossentropy',\n",
    "                                    optimizer=tf.keras.optimizers.legacy.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773465c8-3d55-4c04-beb8-b72983708047",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.7. Build and train models for multi-class categorization.\n",
    "\n",
    "Multi-class categorization is when the output is more than two options. This requires converting each category into a numerical representation.\n",
    "\n",
    "* Output Shape = `(num_classes,)`\n",
    "* Output Activation Function = `softmax`\n",
    "* Loss Function = `categorical_crossentropy` if one-hot encoded format; `spare_categorical_crossentropy` if label encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3be59737-f9e9-445b-bdcd-e8a75b7246fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.constant(['dog', 'cat', 'bird', 'dog'])\n",
    "total_categories = 3  # dog, cat, bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "71a059cc-b70a-4104-93ea-2a87105a9727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding Labels (With sklearn)\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8b90bb2b-9a76-4601-acae-584b59b2c16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
       " array([[0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.]], dtype=float32)>,\n",
       " array([[0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.]]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot-Encoding Labels (With TensorFlow)\n",
    "one_hot_encoded_labels_tf = tf.one_hot(encoded_labels, total_categories)\n",
    "\n",
    "# One-Hot-Encoding Labels (With sklearn)\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_encoded_labels_sklearn = one_hot_encoder.fit_transform(encoded_labels.reshape(-1, 1))\n",
    "\n",
    "one_hot_encoded_labels_tf, one_hot_encoded_labels_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "985f6474-99b2-468e-aa66-5281e6066fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class Model\n",
    "multiclass_classification_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(total_categories, activation='sigmoid'),\n",
    "], name='binary_classification')\n",
    "\n",
    "# Multi-class Model Compiling\n",
    "\n",
    "# Using with one_hot_encoded_labels_tf when fitting\n",
    "multiclass_classification_model.compile(loss='categorical_crossentropy',\n",
    "                                        optimizer=tf.keras.optimizers.legacy.Adam())\n",
    "\n",
    "# Using with encoded_labels when fitting\n",
    "multiclass_classification_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                                        optimizer=tf.keras.optimizers.legacy.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf054d5a-0048-4272-ae9c-f0de2ab63b5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.8. Plot loss and accuracy of a trained model.\n",
    "\n",
    "When fitting a model, a history object is returned with the designated metrics the model was compiled with, along with the loss. This can be plotted using matplotlib.\n",
    "\n",
    "* [History Object](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History): The History object is associated as a callback that is automatically applied to the model when fitting and is returned by the fit method.\n",
    "\n",
    "Some things to consider for the loss curves include:\n",
    "* Ideally, the loss curve vor the validation loss and training loss will follow each other\n",
    "* Point where training loss continues to decrease, but validation loss starts to stabilize implies `overfitting` (see section 2.9 below).\n",
    "\n",
    "**NOTE** I have a personal toolbox, [py-learning-toolbox](https://github.com/bkubick/py-learning-toolbox), that will plot the histroy curve using the function, `ml_toolbox.analysis.history.plot_history`, and the designated metric desired to plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a028b-2dfe-4970-a93d-8844fa45ca77",
   "metadata": {},
   "source": [
    "![image](screenshots/loss_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a5fa4b-f22a-4a7b-92b0-7521582dda0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.9. Identify strategies to prevent overfitting, including augmentation and dropout.\n",
    "\n",
    "Overfitting occurs when the model learns the training data too well, that it can't predict test data or validation well. There are a handful of things that can be done to mitigate/prevent overfitting [Prevent Overfitting](https://www.kdnuggets.com/2019/12/5-techniques-prevent-overfitting-neural-networks.html).\n",
    "\n",
    "* **Decrease complexity of model**: Not much to this, and not much to do to determine best route of simplifying model. Ultimately, just want to reduce the number of trainable parameters in the model.\n",
    "* **Early Stopping**: A callback that can be implemented to stop the model from training once the validation loss is no longer decreasing.\n",
    "* **More Data**: An obvious thing to do is add more data, but this isn't always possible.\n",
    "* **Data Augmentation**: Transforming data to expand the dataset, and give a model various ways of viewing data. Specifically for image classification, popular data augmentation techniques include flipping image, rotating image, zoom image, etc.\n",
    "* **Regularization**: A technique used to reduce complexity of a model through adding penalty terms to the loss function. This is done by adding regularizers to the corresponding layer through `kernel_regularizer`, `bias_regularizer`, or `activity_regularizer` kwarg.\n",
    "  \n",
    "    | L1 Regularization                                | L2 Regularization                         |\n",
    "    | :-----------------                               | :----------------                         |\n",
    "    | Penalizes sum of absolute values of weights      | Penalizes sum of square values of weights |\n",
    "    | Generates model that is simple and interpretable | Able to learn complex data patterns       |\n",
    "    | Robust to outliers                               | Not robust to outliers                    |\n",
    "\n",
    "* **Dropout**: A technique that deactivates random neurons in the model while training, forcing each neuron to \"learn\", in theory, making each neuron more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "52b69b41-2abd-4e26-b29e-cb4bfc6cdd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Callback\n",
    "# `patience`: how many epochs without improvement before ending training\n",
    "# `start_from_epoch`: which epoch to start looking at stopping\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=10, start_from_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9ac3876d-dce9-4b0b-ab5f-7c94540b163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation ImageDataGenerator (Not Preferred)\n",
    "data_augmented_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# Data Augmentation (Preferred Method)\n",
    "data_augmentation_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomHeight(0.2),\n",
    "    tf.keras.layers.RandomWidth(0.2),\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "], name='DataAugmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dfee9210-ec86-4ed2-b711-a66f291292bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization\n",
    "regularization_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(5,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l1(0.01),\n",
    "                          bias_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                          activity_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01)),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "074c1c4f-6190-4181-b9b6-c24936442882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "dropout_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(5),\n",
    "    tf.keras.layers.Dropout(.2),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c7c6d-97a7-4b4e-80ac-94ffa6b20287",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.10. Use pretrained models (transfer learning).\n",
    "\n",
    "Transfer learning can be done by utilizing state of the art architectures trained on signifcant data through various studies, allowing for powerful models without the need for extreme training.\n",
    "\n",
    "TensorFlow Hub provides various models that can be used for transfer learning.\n",
    "* https://www.tensorflow.org/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7263dd97-4d9d-477f-967e-7edbee418163",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7860e9-b80c-4d8a-b628-da95cf3a2d56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.11. Extract features from pre-trained models.\n",
    "Feature extraction in transfer learning is when you take the underlying patterns (also called weights) a pretrained model has learned and adjust its outputs to be more suited to your problem. [[D. Bourke, Zero to Mastery](https://dev.mrdbourke.com/tensorflow-deep-learning/04_transfer_learning_in_tensorflow_part_1_feature_extraction/)]\n",
    "\n",
    "In short, use transfer learning as a non-trainable base model, then pass the output of that to one or multiple layers dedicated to a specific problem (i.e. image classification by taking a model that knows 1000 foods, and add output layer to limit it to a set of only 10 foods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6b8e6fae-cc44-4ada-9d0e-dbc3517e1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "x = base_model(inputs)\n",
    "outputs = tf.keras.layers.Dense(5)(x)  # Feature Extraction Used to dictate the outputs\n",
    "\n",
    "model = tf.keras.models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a1185-b670-4d65-9074-d0aff9759430",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.12. Ensure that inputs to a model are in the correct shape.\n",
    "\n",
    "Inputs to a model are the specific parameters associated with the problem that are used to predict the output. In TensorFlow, the `shape` keyword to each layer automatically applies batching to the data ahead of time, meaning all that has to be specified is the shape of the inputs.\n",
    "\n",
    "| Data Type      | Input Shape                                      |\n",
    "| :--------------| :------------------------------------------------|\n",
    "| Image          | (image height, image width, number of channels)  |\n",
    "| Text           | (1,) - the string of the text                    |\n",
    "| Sequence       | (number of sequence steps, number of features)   |\n",
    "| Feed Forward   | (features)                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ba2def3a-70f9-47e5-849d-e85ee8ce8465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([None, 224, 224, 3]),\n",
       " TensorShape([None, 1]),\n",
       " TensorShape([None, 7]),\n",
       " TensorShape([None, 2]))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_input = tf.keras.layers.Input(shape=(224, 224, 3))\n",
    "text_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "sequence_input = tf.keras.layers.Input(shape=(7,))  # Window size of 7, 0 additional features\n",
    "feed_forward = tf.keras.layers.Input(shape=(2,))  # two features used to predict\n",
    "\n",
    "image_input.shape, text_input.shape, sequence_input.shape, feed_forward.shape  # The None is held for the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a939da-c04b-4951-9f66-ed948e59d121",
   "metadata": {},
   "source": [
    "#### 2.13. Ensure that you can match test data to the input shape of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1572fe77-47b4-495e-804f-4299da0f1ce0",
   "metadata": {},
   "source": [
    "#### 2.14. Ensure you can match output data of a neural network to specified input shape for test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294089d6-4d33-4962-b63e-c2c2260a3c83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.15. Understand batch loading of data.\n",
    "\n",
    "Batching is the process of using multiple samples of data at a time while training, to provide memory efficiency, faster training, and improved generalization. This is done in one of two ways in TensorFlow, while fitting, or through the dataset itself.\n",
    "\n",
    "**NOTE** batch sizes in multiples of 8 typically work better with GPU's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "df5886ee-0fff-4d68-a25a-60ddd30a4ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x1551f4160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x1551f4160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x155259a00>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting with batches\n",
    "# Build Model (Sequential API)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(5),\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(loss='mae', optimizer=tf.keras.optimizers.legacy.Adam())\n",
    "\n",
    "# Train Model\n",
    "model.fit(\n",
    "    x=[1],\n",
    "    y=[[1,2, 3, 4, 5]],\n",
    "    epochs=1,\n",
    "    batch_size=1,  # Batch Size\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9fe32801-90dd-47a5-89d9-0f14616f7413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=TensorSpec(shape=(None,), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batching with dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 1, 33, 5, 3])\n",
    "batched_dataset = dataset.batch(3)\n",
    "batched_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279b74c-fc6c-453f-9eda-ab005d615a8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.16. Use callbacks to trigger the end of training cycles.\n",
    "\n",
    "Callbacks are used when training a model, and are ran at the end of each epoch. They can be used for saving, logging, adjusting learning rates, ending training, etc.\n",
    "\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f04dfceb-9229-459b-8ba7-87738737cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to stop training once model stops improving\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping()\n",
    "\n",
    "# Used to store the model at the end of each epoch as a checkpoint.\n",
    "# This is useful when needing to stop training early and pickup where you left off.\n",
    "# Also useful to save the best trained model\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('./checkpoints')\n",
    "\n",
    "# Logs the metrics for each epoch to the corresponding CSV.\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('./logs/history.csv')\n",
    "\n",
    "# Reduces the learning rate by a factor when the learning stops (hits a plateau)\n",
    "lr_reducer = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "# LR Scheduler is used to update the learning rate for each epoch\n",
    "# This is useful when triying to determine the optimal learning rate\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch, lr: lr * 10 ** 2)\n",
    "\n",
    "# Custom callback\n",
    "lambda_callback = tf.keras.callbacks.LambdaCallback(on_batch_begin=lambda batch,logs: print(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1f4208-3747-4270-a588-a909697ca46f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.17. Use datasets from different sources.\n",
    "\n",
    "Dataset can come from many different areas: tensorflow datasets, github, kaggle, pdfs, documents, raw websites, etc. The key to working with each dataset is to understand what the data is through visualization.\n",
    "\n",
    "I listed a few ways of downloading datasets listed below for various formats.\n",
    "\n",
    "* NOTE: for `.txt` file types, I have a file reader utility in my `py-learning-toolbox` repository: `data_toolbox.read_txt_file_from_url` or `data_toolbox.read_txt_file_from_directory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19a98d45-ddf6-4b53-9871-418f50e205c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 10:31:40.640145: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /Users/brandonkubick/tensorflow_datasets/mnist/3.0.1...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 100%|████████████████████████████| 5/5 [00:01<00:00,  4.38 file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDataset mnist downloaded and prepared to /Users/brandonkubick/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow Datasets\n",
    "ds = tfds.load('mnist', split='train', shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07780ee4-465a-4c64-88f7-a9197470dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Keras\n",
    "ds = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c409b5b-a3e5-4f9c-8302-822477563770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset from the raw csv file on the public github file\n",
    "insurance = pd.read_csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv')\n",
    "insurance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b7f9ee7-ad93-4743-a95e-39048e82177b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>WARNING_UPGRADE_TO_V6</th>\n",
       "      <th>terms</th>\n",
       "      <th>base</th>\n",
       "      <th>date</th>\n",
       "      <th>time_last_updated</th>\n",
       "      <th>rates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AED</th>\n",
       "      <td>https://www.exchangerate-api.com</td>\n",
       "      <td>https://www.exchangerate-api.com/docs/free</td>\n",
       "      <td>https://www.exchangerate-api.com/terms</td>\n",
       "      <td>USD</td>\n",
       "      <td>2023-10-03</td>\n",
       "      <td>1696291201</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFN</th>\n",
       "      <td>https://www.exchangerate-api.com</td>\n",
       "      <td>https://www.exchangerate-api.com/docs/free</td>\n",
       "      <td>https://www.exchangerate-api.com/terms</td>\n",
       "      <td>USD</td>\n",
       "      <td>2023-10-03</td>\n",
       "      <td>1696291201</td>\n",
       "      <td>78.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALL</th>\n",
       "      <td>https://www.exchangerate-api.com</td>\n",
       "      <td>https://www.exchangerate-api.com/docs/free</td>\n",
       "      <td>https://www.exchangerate-api.com/terms</td>\n",
       "      <td>USD</td>\n",
       "      <td>2023-10-03</td>\n",
       "      <td>1696291201</td>\n",
       "      <td>100.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMD</th>\n",
       "      <td>https://www.exchangerate-api.com</td>\n",
       "      <td>https://www.exchangerate-api.com/docs/free</td>\n",
       "      <td>https://www.exchangerate-api.com/terms</td>\n",
       "      <td>USD</td>\n",
       "      <td>2023-10-03</td>\n",
       "      <td>1696291201</td>\n",
       "      <td>401.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANG</th>\n",
       "      <td>https://www.exchangerate-api.com</td>\n",
       "      <td>https://www.exchangerate-api.com/docs/free</td>\n",
       "      <td>https://www.exchangerate-api.com/terms</td>\n",
       "      <td>USD</td>\n",
       "      <td>2023-10-03</td>\n",
       "      <td>1696291201</td>\n",
       "      <td>1.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             provider  \\\n",
       "AED  https://www.exchangerate-api.com   \n",
       "AFN  https://www.exchangerate-api.com   \n",
       "ALL  https://www.exchangerate-api.com   \n",
       "AMD  https://www.exchangerate-api.com   \n",
       "ANG  https://www.exchangerate-api.com   \n",
       "\n",
       "                          WARNING_UPGRADE_TO_V6  \\\n",
       "AED  https://www.exchangerate-api.com/docs/free   \n",
       "AFN  https://www.exchangerate-api.com/docs/free   \n",
       "ALL  https://www.exchangerate-api.com/docs/free   \n",
       "AMD  https://www.exchangerate-api.com/docs/free   \n",
       "ANG  https://www.exchangerate-api.com/docs/free   \n",
       "\n",
       "                                      terms base       date  \\\n",
       "AED  https://www.exchangerate-api.com/terms  USD 2023-10-03   \n",
       "AFN  https://www.exchangerate-api.com/terms  USD 2023-10-03   \n",
       "ALL  https://www.exchangerate-api.com/terms  USD 2023-10-03   \n",
       "AMD  https://www.exchangerate-api.com/terms  USD 2023-10-03   \n",
       "ANG  https://www.exchangerate-api.com/terms  USD 2023-10-03   \n",
       "\n",
       "     time_last_updated   rates  \n",
       "AED         1696291201    3.67  \n",
       "AFN         1696291201   78.23  \n",
       "ALL         1696291201  100.95  \n",
       "AMD         1696291201  401.90  \n",
       "ANG         1696291201    1.79  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currency = pd.read_json('https://api.exchangerate-api.com/v4/latest/USD')\n",
    "currency.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0d65b8-f9d1-4d19-8c3f-0607265bcdee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.18. Use datasets in different formats, including json and csv.\n",
    "\n",
    "See section 2.17. above about how to use datasets from different resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9abb5e5-e2e8-4e0b-9ac7-7ab297e29a5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.19. Use datasets from tf.data.datasets.\n",
    "\n",
    "This is actually not the case, tensorflow datasets is the primary source (`tensorflow_datasets`).\n",
    "\n",
    "* https://www.tensorflow.org/datasets/performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba123bb-97ff-4ef8-a493-533a89deff11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. Image classification\n",
    "\n",
    "You need to understand how to build image recognition and object detection models with deep neural networks and convolutional neural networks using TensorFlow 2.x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ac9dc-433c-468a-afa0-4fc742941594",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3.1. Define Convolutional neural networks with Conv2D and pooling layers.\n",
    "\n",
    "Convolution layers are the standard layers in image classification which implement the Convolutional Neural Network Architecture.\n",
    "\n",
    "* `Conv2D` is the standard convolutional layer for image classification problems used to **detect features**. This is a condensed layer that incorporates the standard CNN architecture.\n",
    "* `Conv2DTranspose` is the standard layer for deconvolution used to **create features** by dispersing the detected features to a broader area.\n",
    "    * NOTE: This is used typically in autoencoder problems\n",
    "\n",
    "Pooling layers are typically used in conjunctions with convolutional layers which have many functions, but primarily are used to either smooth out or sharpen prominant features depending on the pooling used.\n",
    "\n",
    "* `AveragePooling2D` is a less used pooling layer that is used to smooth out features by averaging the patch of features and reducing typically a 2x2 patch to a 1x1 cell.\n",
    "* `MaxPooling2D` is most commonly used as the pooling layer in CNN architectures due to it sharpening prominant features through taking the max value in a 2x2 patch, and reducing the patch to a 1x1 cell with the max value.\n",
    "* `GlobalMaxPooling2D`/`GlobalAveragePooling2D` is less likely used for image classification, but is used with CNN architectures and instead of reducing and NxM matrix to a N/2xM/2 matrix, it flattens the matrix into a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7de11fd-4edf-41ec-ac22-f97da702628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_layer = tf.keras.layers.Conv2D(filters=5, kernel_size=3, padding='valid')\n",
    "conv2d_transpose_layer = tf.keras.layers.Conv2DTranspose(filters=5, kernel_size=3, padding='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48cbfd3a-76e3-4245-8da5-76c10472d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_pooling_2d_layer = tf.keras.layers.AveragePooling2D()\n",
    "max_pooling_2d_layer = tf.keras.layers.MaxPooling2D()\n",
    "global_average_pooling_2d_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "global_max_pooling_2d_layer = tf.keras.layers.GlobalMaxPooling2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79df9a5-9abc-423f-993e-c5ebbbbec3c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3.3. Understand how to use convolutions to improve your neural network.\n",
    "\n",
    "Convolutional Neural Networks are like all other neural networks when it comes to improving.\n",
    "1. More Data (if not possible, then Augment data)\n",
    "2. Tune Hyper Parameters\n",
    "3. Reduce Over/under fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62842f8c-e1c0-4044-82dc-6d6b33b42b4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3.4. Use real-world images in different shapes and sizes.\n",
    "\n",
    "The whole purpose of building out these models is to be able to take a real world image, and predict what it is. To do this however, requires ensuring that the image matches the parameters associated with the model.\n",
    "\n",
    "For instance, if an image comes in that is of size 1400x1400 pixels, but you model was trained on images with 224x224 pixels, this leads to shape errors when trying to input the data. To fix this, we typically need to preprocess an image before passing it to the model\n",
    "\n",
    "&#128273; **NOTE**: I have a function in my personal `py-learning-toolbox` repository that loads and preprocesses an image directly (`ml_toolbox.preprocessing.image.load_and_resize_image`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26b577e-7c80-4b17-995b-bbf3fe1f6712",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3.5. Use image augmentation to prevent overfitting.\n",
    "Data Augmentation is the process of transforming data to expand the dataset, and give a model various ways of viewing data. Specifically for image classification, popular data augmentation techniques include flipping image, rotating image, zoom image, etc.\n",
    "\n",
    "&#128273; **Note**: Data augmentation is inactive at test time so input images will only be augmented during calls to Model.fit (not Model.evaluate or Model.predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b49ca97-c654-42b5-8178-33ba67a2a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation ImageDataGenerator (Not Preferred)\n",
    "data_augmented_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# Data Augmentation (Preferred Method)\n",
    "data_augmentation_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomHeight(0.2),\n",
    "    tf.keras.layers.RandomWidth(0.2),\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "], name='DataAugmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e9324d-e824-46c1-955b-f466d5c16833",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3.6. Use ImageDataGenerator.\n",
    "\n",
    "The `ImageDataGenerator` has since been deprecated in favor of the `tf.keras.utils.image_dataset_from_directory`, but is required to learn as part of the exam.\n",
    "\n",
    "The `ImageDataGenerator` flows in image files from a corresponding directory in batches. The purpose of this is to limit the amount of data in memory at the time of accessing each image, along with providing an easy way to augment the data directly (see section 3.5. above for details on image augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9770faa6-96a1-4283-ae2c-1e2f5f5b736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation ImageDataGenerator (Not Preferred)\n",
    "image_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "data = image_data_generator.flow_from_directory('./data/sample_food/train',\n",
    "                                                target_size=(224, 224),\n",
    "                                                batch_size=1,\n",
    "                                                class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e198f2c-8df7-4869-a5c6-ce20353e0fae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3.7. Understand how ImageDataGenerator labels images based on the directory structure.\n",
    "\n",
    "The `ImageDataGenerator` labels images based on the directory structure by the following:\n",
    "\n",
    "```\n",
    "├── test\n",
    "│   ├── cheese\n",
    "│   │   ├── 000001.png\n",
    "│   │   ├── ...\n",
    "│   ├── carrot\n",
    "│   │   ├── 000001.png\n",
    "│   │   ├── ...\n",
    "├── train\n",
    "│   ├── cheese\n",
    "│   │   ├── 000002.png\n",
    "│   │   ├── ...\n",
    "│   ├── carrot\n",
    "│   │   ├── 000001.png\n",
    "│   │   ├── ...\n",
    "\n",
    "```\n",
    "\n",
    "It then maps each image name to the parent directory (i.e. cheese or carrot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "269ecdce-9cf3-4475-be44-4e63a3af8d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'carrot': 0, 'cheese': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Augmentation ImageDataGenerator (Not Preferred)\n",
    "image_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "data = image_data_generator.flow_from_directory('./data/sample_food/train',\n",
    "                                                target_size=(224, 224),\n",
    "                                                batch_size=1,\n",
    "                                                class_mode='categorical')\n",
    "data.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454c1cd-d3e8-4b85-adaa-24f923799595",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4. Natural language processing (NLP)\n",
    "\n",
    "You need to understand how to use neural networks to solve natural language processing problems using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd965bad-d15e-4ded-8c0e-a3760573b1c5",
   "metadata": {},
   "source": [
    "#### 4.1. Build natural language processing systems using TensorFlow.\n",
    "\n",
    "NLP is taking language, and using it to predict things such as text generation, text classification, text translation, etc. This falls into a sequence to sequence problem, and RNN or Conv1D networks can be used to build out these models.\n",
    "\n",
    "* https://www.tensorflow.org/tutorials/text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ec7750-1446-4f59-9847-f9dfc85b04dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.2. Prepare text to use in TensorFlow models.\n",
    "\n",
    "In addition to removing and formatting any characters you may not want to worry about in your data, text needs to be converted into a numerical format in order for it to be modeled.\n",
    "\n",
    "* Text Vectorization: Process of converting text into numerical format\n",
    "* Token Embedding: Process of vectorizing individual tokens into learnable parameters. This is typically used in Language Processing rather than One-hot-encoded text variables because it provides learnable parameters, and due to the vast number of words, we won't be working with huge OneHotEncoder matrices.\n",
    "    * i.e. Embedding the word family might look something like:\n",
    "        * `family` --> `text_vectorizer('family')` --> `127` --> `token_embedding(127)` --> `[.853123, .34123, .049123, ....]`\n",
    "    * The above example converts the word `family` into an n length vector of numerical entries.\n",
    "* One Hot Encoder: Process of converting text values into numerical values be specifying a new iput variable for each unique word. Due to there being a new variable for each unique word, One Hot Encoders are not often used with LLM aside from determining the output variable because it can results in huge amounts of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120845f7-df0a-4cd7-8d83-dd887eba6843",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b478b3f5-8e79-41be-9b87-cff0dadc3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "sentences = ['You have done well my padawan.', \"You're a wizard Harry.\", \"Get to the chopper, the building is on fire!\"]\n",
    "sample_sentence = ['Harry is a padawan wizard.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "234bb9d0-94a7-4b59-b678-c17505cc3592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 12), dtype=int64, numpy=array([[13, 11, 19,  8,  5,  0,  0,  0,  0,  0,  0,  0]])>,\n",
       " ['',\n",
       "  '[UNK]',\n",
       "  'the',\n",
       "  'youre',\n",
       "  'you',\n",
       "  'wizard',\n",
       "  'well',\n",
       "  'to',\n",
       "  'padawan',\n",
       "  'on',\n",
       "  'my',\n",
       "  'is',\n",
       "  'have',\n",
       "  'harry',\n",
       "  'get',\n",
       "  'fire',\n",
       "  'done',\n",
       "  'chopper',\n",
       "  'building',\n",
       "  'a'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Vectorization\n",
    "word_vectorizer = tf.keras.layers.TextVectorization(max_tokens=1000,\n",
    "                                                    output_sequence_length=12,\n",
    "                                                    standardize='lower_and_strip_punctuation')\n",
    "word_vectorizer.adapt(sentences)\n",
    "\n",
    "word_vectorizer(sample_sentence), word_vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "480c848a-936b-4f21-a1d2-2d4417fb5952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 12), dtype=int64, numpy=array([[ 8,  4,  5,  5,  9,  2,  7, 20,  2,  4,  2, 15]])>,\n",
       " ['',\n",
       "  '[UNK]',\n",
       "  ' ',\n",
       "  'e',\n",
       "  'a',\n",
       "  'r',\n",
       "  'o',\n",
       "  'i',\n",
       "  'h',\n",
       "  'y',\n",
       "  't',\n",
       "  'n',\n",
       "  'd',\n",
       "  'w',\n",
       "  'u',\n",
       "  'p',\n",
       "  'l',\n",
       "  'g',\n",
       "  'z',\n",
       "  'v',\n",
       "  's',\n",
       "  'm',\n",
       "  'f',\n",
       "  'c',\n",
       "  'b'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vectorizer = tf.keras.layers.TextVectorization(max_tokens=1000,\n",
    "                                                    output_sequence_length=12,\n",
    "                                                    standardize='lower_and_strip_punctuation',\n",
    "                                                    split='character')\n",
    "char_vectorizer.adapt(sentences)\n",
    "\n",
    "char_vectorizer(sample_sentence), char_vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a32fd-4fc6-4539-8605-ea0cba402619",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3193e9fc-946d-4fd7-843e-74c2aef02f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy=\n",
       "array([[[ 0.04119045, -0.04244716,  0.0028836 , -0.04536789,\n",
       "          0.01513002],\n",
       "        [-0.03763362,  0.01767519, -0.03392949, -0.0370407 ,\n",
       "         -0.03699964],\n",
       "        [-0.00488741,  0.00283382,  0.04399434,  0.04421136,\n",
       "         -0.01885287],\n",
       "        [-0.03914008,  0.02328603, -0.02744038,  0.02392345,\n",
       "          0.04472652],\n",
       "        [ 0.03515441,  0.04961933, -0.02426612, -0.02699971,\n",
       "          0.01845104]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(word_vectorizer.get_vocabulary()),\n",
    "    output_dim=5,\n",
    "    mask_zero=True)\n",
    "\n",
    "token_embedding(word_vectorizer(sample_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbbea7ac-b819-4eaa-8acf-3471d58e649a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 25, 5), dtype=float32, numpy=\n",
       "array([[[-0.04124928, -0.00360743,  0.04356502, -0.02659626,\n",
       "          0.03048421],\n",
       "        [-0.04939149,  0.01697184, -0.00858854,  0.0381493 ,\n",
       "         -0.03224466],\n",
       "        [-0.03248235,  0.02507995,  0.03194323,  0.03108955,\n",
       "         -0.00893488],\n",
       "        [-0.03248235,  0.02507995,  0.03194323,  0.03108955,\n",
       "         -0.00893488],\n",
       "        [ 0.00541637,  0.01810095,  0.03862338, -0.00354055,\n",
       "         -0.0013557 ],\n",
       "        [-0.00377619, -0.03032162,  0.01447741,  0.03560669,\n",
       "         -0.02133771],\n",
       "        [-0.00211342, -0.01595372, -0.03345835, -0.03977276,\n",
       "         -0.02123622],\n",
       "        [ 0.03083609,  0.02962705, -0.01760997,  0.0243361 ,\n",
       "          0.04298132],\n",
       "        [-0.00377619, -0.03032162,  0.01447741,  0.03560669,\n",
       "         -0.02133771],\n",
       "        [-0.04939149,  0.01697184, -0.00858854,  0.0381493 ,\n",
       "         -0.03224466],\n",
       "        [-0.00377619, -0.03032162,  0.01447741,  0.03560669,\n",
       "         -0.02133771],\n",
       "        [ 0.04196498, -0.04177827,  0.01494166,  0.02765092,\n",
       "          0.00996578],\n",
       "        [-0.04939149,  0.01697184, -0.00858854,  0.0381493 ,\n",
       "         -0.03224466],\n",
       "        [ 0.03062372,  0.01059836, -0.01960346, -0.01384038,\n",
       "          0.02159411],\n",
       "        [-0.04939149,  0.01697184, -0.00858854,  0.0381493 ,\n",
       "         -0.03224466],\n",
       "        [-0.03450589,  0.00434575, -0.00987768, -0.03023509,\n",
       "          0.04337524],\n",
       "        [-0.04939149,  0.01697184, -0.00858854,  0.0381493 ,\n",
       "         -0.03224466],\n",
       "        [ 0.031776  , -0.00704835, -0.00919528, -0.02283589,\n",
       "          0.04955346],\n",
       "        [-0.00377619, -0.03032162,  0.01447741,  0.03560669,\n",
       "         -0.02133771],\n",
       "        [-0.03450589,  0.00434575, -0.00987768, -0.03023509,\n",
       "          0.04337524],\n",
       "        [-0.00211342, -0.01595372, -0.03345835, -0.03977276,\n",
       "         -0.02123622],\n",
       "        [ 0.02275607,  0.0242036 , -0.03185989,  0.03454334,\n",
       "         -0.0007324 ],\n",
       "        [-0.04939149,  0.01697184, -0.00858854,  0.0381493 ,\n",
       "         -0.03224466],\n",
       "        [-0.03248235,  0.02507995,  0.03194323,  0.03108955,\n",
       "         -0.00893488],\n",
       "        [ 0.03062372,  0.01059836, -0.01960346, -0.01384038,\n",
       "          0.02159411]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(char_vectorizer.get_vocabulary()),\n",
    "    output_dim=5,\n",
    "    mask_zero=True)\n",
    "\n",
    "char_embedding(char_vectorizer(sample_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d1143-9089-4e9f-afed-82fff6b3f937",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.3. Build models that identify the category of a piece of text using binary categorization\n",
    "\n",
    "* Binary Classification implies that the output is either 1 or 0.\n",
    "* Text implies text vectorization and token embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21c2da6f-c56c-4d97-9f06-74dfff139d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "sentences = ['You have done well my padawan.', \"You're a wizard Harry.\", \"Get to the chopper, the building is on fire!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25645589-cdfe-4055-8824-952fbff8af99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_15 (Tex  (None, 12)                0         \n",
      " tVectorization)                                                 \n",
      "                                                                 \n",
      " embedding_9 (Embedding)     (None, 12, 5)             100       \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 8)                 448       \n",
      "                                                                 \n",
      " binary_output (Dense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 557 (2.18 KB)\n",
      "Trainable params: 557 (2.18 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Word Vectorizer\n",
    "word_vectorizer = tf.keras.layers.TextVectorization(max_tokens=1000,\n",
    "                                                    output_sequence_length=12,\n",
    "                                                    standardize='lower_and_strip_punctuation')\n",
    "word_vectorizer.adapt(sentences)\n",
    "\n",
    "# Word Embedding\n",
    "token_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(word_vectorizer.get_vocabulary()),\n",
    "    output_dim=5,\n",
    "    mask_zero=True)\n",
    "\n",
    "# Model with Embedings\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = word_vectorizer(inputs)\n",
    "x = token_embedding(x)\n",
    "x = tf.keras.layers.LSTM(8)(x)\n",
    "outputs = tf.keras.layers.Dense(1, name='binary_output')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db0d43-bafa-4dfb-be9f-02d4e63e555c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.4. Build models that identify the category of a piece of text using multi-class categorization\n",
    "\n",
    "* Multi-class Classification implies that the output is one output of multiple possibilities.\n",
    "    * This means the output needs to be one-hot-encoded (`categorical_crossentropy` as loss function).\n",
    "* Text implies text vectorization and token embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11b3613d-5431-4023-ab66-c4c9edb9a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['You have done well my padawan.', \"You're a wizard Harry.\", \"Get to the chopper, the building is on fire!\"]\n",
    "output = ['Star Wars', 'Harry Potter', 'Arnold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba11b7ae-0879-47e5-80e5-8b9735370fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_20 (Tex  (None, 12)                0         \n",
      " tVectorization)                                                 \n",
      "                                                                 \n",
      " embedding_14 (Embedding)    (None, 12, 5)             100       \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 8)                 448       \n",
      "                                                                 \n",
      " binary_output (Dense)       (None, 3)                 27        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 575 (2.25 KB)\n",
      "Trainable params: 575 (2.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encoded Output (Used in Fit function)\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_labels = one_hot_encoder.fit_transform(np.array(output).reshape((-1,1)))\n",
    "\n",
    "# Word Vectorizer\n",
    "word_vectorizer = tf.keras.layers.TextVectorization(max_tokens=1000,\n",
    "                                                    output_sequence_length=12,\n",
    "                                                    standardize='lower_and_strip_punctuation')\n",
    "word_vectorizer.adapt(sentences)\n",
    "\n",
    "# Word Embedding\n",
    "token_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(word_vectorizer.get_vocabulary()),\n",
    "    output_dim=5,\n",
    "    mask_zero=True)\n",
    "\n",
    "# Model with Embedings\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = word_vectorizer(inputs)\n",
    "x = token_embedding(x)\n",
    "x = tf.keras.layers.LSTM(8)(x)\n",
    "outputs = tf.keras.layers.Dense(3, name='binary_output')(x)  # There are 3 categories\n",
    "\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe081b7-93d7-413d-9565-20d8fb4cb3e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.5. Use word embeddings in your TensorFlow model.\n",
    "\n",
    "See section 4.2 going into embeddings and why they are needed. See below for how to implement them into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "150df66c-adb4-4a20-9a07-d4187ddf7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "sentences = ['You have done well my padawan.', \"You're a wizard Harry.\", \"Get to the chopper, the building is on fire!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f21e2714-f594-4b12-82ce-b56670652ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_11 (Tex  (None, 12)                0         \n",
      " tVectorization)                                                 \n",
      "                                                                 \n",
      " embedding_7 (Embedding)     (None, 12, 5)             100       \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 8)                 448       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 584 (2.28 KB)\n",
      "Trainable params: 584 (2.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Word Vectorizer\n",
    "word_vectorizer = tf.keras.layers.TextVectorization(max_tokens=1000,\n",
    "                                                    output_sequence_length=12,\n",
    "                                                    standardize='lower_and_strip_punctuation')\n",
    "word_vectorizer.adapt(sentences)\n",
    "\n",
    "# Word Embedding\n",
    "token_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(word_vectorizer.get_vocabulary()),\n",
    "    output_dim=5,\n",
    "    mask_zero=True)\n",
    "\n",
    "# Model with Embedings\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = word_vectorizer(inputs)\n",
    "x = token_embedding(x)\n",
    "x = tf.keras.layers.LSTM(8)(x)\n",
    "outputs = tf.keras.layers.Dense(4)(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728ee55-f39c-484e-b5d2-774df5d2816c",
   "metadata": {},
   "source": [
    "#### 4.6. Use LSTMs in your model to classify text for either binary or multi-class categorization.\n",
    "\n",
    "See sectins 4.3-4.5 for examples of using LSTM's as well.\n",
    "\n",
    "&#128273; **NOTE**:  LSTM and GRU layers work best with the `tanh` activation function (default for TensorFlow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05717c1a-b755-4929-ab3d-f3f5d0c36052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "sentences = ['You have done well my padawan.', \"You're a wizard Harry.\", \"Get to the chopper, the building is on fire!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66833b-2f08-461f-90fd-4422f20deea2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Single LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306b4e72-0260-4d2a-b6ba-b270446e6438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Vectorizer\n",
    "word_vectorizer = tf.keras.layers.TextVectorization(max_tokens=1000,\n",
    "                                                    output_sequence_length=12,\n",
    "                                                    standardize='lower_and_strip_punctuation')\n",
    "word_vectorizer.adapt(sentences)\n",
    "\n",
    "# Word Embedding\n",
    "token_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(word_vectorizer.get_vocabulary()),\n",
    "    output_dim=5,\n",
    "    mask_zero=True)\n",
    "\n",
    "# Model with Embedings\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = word_vectorizer(inputs)\n",
    "x = token_embedding(x)\n",
    "x = tf.keras.layers.LSTM(8)(x)\n",
    "outputs = tf.keras.layers.Dense(4)(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ac929-d0bb-4440-b757-4e8bc7e2349c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Stacked LSTM Layers\n",
    "\n",
    "* `return_sequences=True` needs to be set for every stacked LSTM layer, aside from the last LSTM layer.\n",
    "\n",
    "&#128273; **NOTE**: Simple Networks, a single LSTM is usually sufficient, and 2 LSTM layers can typically handle more complex patterns. In most cases, this is all you will need. More than 2 LSTM layers can improve models, but at the cost of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47a2131b-7104-403e-80b8-aa3d7f3ef164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_23 (Tex  (None, 12)                0         \n",
      " tVectorization)                                                 \n",
      "                                                                 \n",
      " embedding_17 (Embedding)    (None, 12, 5)             100       \n",
      "                                                                 \n",
      " lstm_22 (LSTM)              (None, 12, 8)             448       \n",
      "                                                                 \n",
      " lstm_23 (LSTM)              (None, 12, 8)             544       \n",
      "                                                                 \n",
      " lstm_24 (LSTM)              (None, 12, 8)             544       \n",
      "                                                                 \n",
      " lstm_25 (LSTM)              (None, 8)                 544       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2216 (8.66 KB)\n",
      "Trainable params: 2216 (8.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Word Vectorizer\n",
    "word_vectorizer = tf.keras.layers.TextVectorization(max_tokens=1000,\n",
    "                                                    output_sequence_length=12,\n",
    "                                                    standardize='lower_and_strip_punctuation')\n",
    "word_vectorizer.adapt(sentences)\n",
    "\n",
    "# Word Embedding\n",
    "token_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(word_vectorizer.get_vocabulary()),\n",
    "    output_dim=5,\n",
    "    mask_zero=True)\n",
    "\n",
    "# Model with Embedings\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = word_vectorizer(inputs)\n",
    "x = token_embedding(x)\n",
    "x = tf.keras.layers.LSTM(8, return_sequences=True)(x)\n",
    "x = tf.keras.layers.LSTM(8, return_sequences=True)(x)\n",
    "x = tf.keras.layers.LSTM(8, return_sequences=True)(x)\n",
    "x = tf.keras.layers.LSTM(8)(x)\n",
    "outputs = tf.keras.layers.Dense(4)(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa67e3-d18e-4b68-94f3-e5340640135c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.7. Add RNN and GRU layers to your model.\n",
    "\n",
    "See section 4.6 for adding LSTM layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "adcd4d03-0b9a-41c8-9384-ab570394998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "sentences = ['You have done well my padawan.', \"You're a wizard Harry.\", \"Get to the chopper, the building is on fire!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3bb37e-fca0-4078-8aea-7d19d02024e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0379cdc9-7e1a-4a9e-913a-77d25a568a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_24 (Tex  (None, 12)                0         \n",
      " tVectorization)                                                 \n",
      "                                                                 \n",
      " embedding_18 (Embedding)    (None, 12, 5)             100       \n",
      "                                                                 \n",
      " lstm_26 (LSTM)              (None, 8)                 448       \n",
      "                                                                 \n",
      " binary_output (Dense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 557 (2.18 KB)\n",
      "Trainable params: 557 (2.18 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Word Vectorizer\n",
    "word_vectorizer = tf.keras.layers.TextVectorization(max_tokens=1000,\n",
    "                                                    output_sequence_length=12,\n",
    "                                                    standardize='lower_and_strip_punctuation')\n",
    "word_vectorizer.adapt(sentences)\n",
    "\n",
    "# Word Embedding\n",
    "token_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(word_vectorizer.get_vocabulary()),\n",
    "    output_dim=5,\n",
    "    mask_zero=True)\n",
    "\n",
    "# Model with Embedings\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = word_vectorizer(inputs)\n",
    "x = token_embedding(x)\n",
    "x = tf.keras.layers.LSTM(8)(x)\n",
    "outputs = tf.keras.layers.Dense(1, name='binary_output')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52213857-7372-4654-b7e1-a4e44ff89068",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### GRU Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4519a2f-c712-4497-a84d-f3beb75b2749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_25 (Tex  (None, 12)                0         \n",
      " tVectorization)                                                 \n",
      "                                                                 \n",
      " embedding_19 (Embedding)    (None, 12, 5)             100       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 8)                 360       \n",
      "                                                                 \n",
      " binary_output (Dense)       (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 469 (1.83 KB)\n",
      "Trainable params: 469 (1.83 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Word Vectorizer\n",
    "word_vectorizer = tf.keras.layers.TextVectorization(max_tokens=1000,\n",
    "                                                    output_sequence_length=12,\n",
    "                                                    standardize='lower_and_strip_punctuation')\n",
    "word_vectorizer.adapt(sentences)\n",
    "\n",
    "# Word Embedding\n",
    "token_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(word_vectorizer.get_vocabulary()),\n",
    "    output_dim=5,\n",
    "    mask_zero=True)\n",
    "\n",
    "# Model with Embedings\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = word_vectorizer(inputs)\n",
    "x = token_embedding(x)\n",
    "x = tf.keras.layers.GRU(8)(x)\n",
    "outputs = tf.keras.layers.Dense(1, name='binary_output')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6babfcf-dbcc-48e3-b1bc-23ed6e241dd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Bidirectional LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3faf59c6-629d-4048-9d56-523a07573437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_26 (Tex  (None, 12)                0         \n",
      " tVectorization)                                                 \n",
      "                                                                 \n",
      " embedding_20 (Embedding)    (None, 12, 5)             100       \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 16)                896       \n",
      " al)                                                             \n",
      "                                                                 \n",
      " binary_output (Dense)       (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1013 (3.96 KB)\n",
      "Trainable params: 1013 (3.96 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Word Vectorizer\n",
    "word_vectorizer = tf.keras.layers.TextVectorization(max_tokens=1000,\n",
    "                                                    output_sequence_length=12,\n",
    "                                                    standardize='lower_and_strip_punctuation')\n",
    "word_vectorizer.adapt(sentences)\n",
    "\n",
    "# Word Embedding\n",
    "token_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(word_vectorizer.get_vocabulary()),\n",
    "    output_dim=5,\n",
    "    mask_zero=True)\n",
    "\n",
    "# Model with Embedings\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = word_vectorizer(inputs)\n",
    "x = token_embedding(x)\n",
    "x = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(8),\n",
    ")(x)\n",
    "outputs = tf.keras.layers.Dense(1, name='binary_output')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8781a17a-e9b6-4aef-adf3-f132693c08e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### RNN Layer\n",
    "\n",
    "An `RNN` layer in TensorFlow must take in a layer that inherits an RNN Cell instance. For example, the following are provided `Cell` layers:\n",
    "* `tf.keras.layers.LSTMCell`\n",
    "* `tf.keras.layers.GRUCell`\n",
    "* `tf.keras.layers.SimpleRNNCell`\n",
    "\n",
    "In order to stack `Cell` instances when building an RNN, you must use the `StackedRNNCells` class with a list of `Cell` instances as it's argument.\n",
    "* `tf.keras.layers.StackedRNNCells`\n",
    "\n",
    "* [TensorFlow Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7c4c4cc-e752-4cf8-95fd-571238a767de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_21 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_28 (Tex  (None, 12)                0         \n",
      " tVectorization)                                                 \n",
      "                                                                 \n",
      " embedding_22 (Embedding)    (None, 12, 5)             100       \n",
      "                                                                 \n",
      " rnn (RNN)                   (None, 5)                 55        \n",
      "                                                                 \n",
      " binary_output (Dense)       (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161 (644.00 Byte)\n",
      "Trainable params: 161 (644.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Word Vectorizer\n",
    "word_vectorizer = tf.keras.layers.TextVectorization(max_tokens=1000,\n",
    "                                                    output_sequence_length=12,\n",
    "                                                    standardize='lower_and_strip_punctuation')\n",
    "word_vectorizer.adapt(sentences)\n",
    "\n",
    "# Word Embedding\n",
    "token_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(word_vectorizer.get_vocabulary()),\n",
    "    output_dim=5,\n",
    "    mask_zero=True)\n",
    "\n",
    "# Model with Embedings\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = word_vectorizer(inputs)\n",
    "x = token_embedding(x)\n",
    "x = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(5))(x)\n",
    "outputs = tf.keras.layers.Dense(1, name='binary_output')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4762456a-7e16-4286-8906-6180ed102d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_23 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_30 (Tex  (None, 12)                0         \n",
      " tVectorization)                                                 \n",
      "                                                                 \n",
      " embedding_24 (Embedding)    (None, 12, 5)             100       \n",
      "                                                                 \n",
      " rnn_1 (RNN)                 (None, 5)                 660       \n",
      "                                                                 \n",
      " binary_output (Dense)       (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 766 (2.99 KB)\n",
      "Trainable params: 766 (2.99 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Word Vectorizer\n",
    "word_vectorizer = tf.keras.layers.TextVectorization(max_tokens=1000,\n",
    "                                                    output_sequence_length=12,\n",
    "                                                    standardize='lower_and_strip_punctuation')\n",
    "word_vectorizer.adapt(sentences)\n",
    "\n",
    "# Word Embedding\n",
    "token_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(word_vectorizer.get_vocabulary()),\n",
    "    output_dim=5,\n",
    "    mask_zero=True)\n",
    "\n",
    "# Model with Embedings\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = word_vectorizer(inputs)\n",
    "x = token_embedding(x)\n",
    "\n",
    "rnn_cells = [tf.keras.layers.LSTMCell(5) for i in range(3)]\n",
    "stacked_rnn_cells = tf.keras.layers.StackedRNNCells(rnn_cells)\n",
    "x = tf.keras.layers.RNN(stacked_rnn_cells)(x)\n",
    "outputs = tf.keras.layers.Dense(1, name='binary_output')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b5b1b-468a-42b8-8303-c7666b20062b",
   "metadata": {},
   "source": [
    "#### 4.8. Use RNNS, LSTMs, GRUs and CNNs in models that work with text.\n",
    "\n",
    "See section 4.7 example for how to build models that work with text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7fd645-03ab-43c2-92eb-34bc06fd6329",
   "metadata": {},
   "source": [
    "#### 4.9. Train LSTMs on existing text to generate text (such as songs and poetry)\n",
    "\n",
    "I did a separate notebook for this.\n",
    "* [My Poetry Generation Notebook](https://github.com/bkubick/deep-learning-development/blob/main/projects/poetry_generator/poetry_generator.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e7eda-f0b8-45b4-86fd-1b8bc061e1af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5. Time series, sequences and predictions\n",
    "\n",
    "You need to understand how to solve time series and forecasting problems in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4b288-5abb-4750-92c6-bcf26e457117",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.1. Train, tune and use time series, sequence and prediction models.\n",
    "\n",
    "Sequence and time series models utilize RNN or CNN architectures, because the order of which the data appears matters.\n",
    "\n",
    "* [Train, tune, and use time series](https://tensorflow.backprop.fr/time-series-sequences-and-predictions/train-tune-and-use-timeseries-sequence-and-prediction-models/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df1ab11-ab5e-44a8-a600-109e86000127",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.2. Train models to predict values for both univariate and multivariate time series.\n",
    "\n",
    "* **Univariate**: The term \"univariate time series\" refers to a time series that consists of single (scalar) observations recorded sequentially over equal time increments.\n",
    "    * This is the time `windows` as done in the Bit Predict notebook, where we took the previous 7 days closing prices, and used it to predict the next day's closing price.\n",
    "    * `[1, 2, 3, 4, 5]` --> `[6]`\n",
    "* **Multivariate**: A Multivariate time series has more than one time series variable. Each variable depends not only on its past values but also has some dependency on other variables. This dependency is used for forecasting future values.\n",
    "    * This is the experiment in Bit Predict, where we used both the time `windows` of closing costs, as well as the block reward for the given day.\n",
    "    * `[1, 2, 3, 4, 5, block_reward]` --> `[6]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da5fb03-8571-4da1-be7b-35a44059e31f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.3. Prepare data for time series learning.\n",
    "\n",
    "Preparing data for time series learning consists of windowing the series for a given window size. For instance:\n",
    "* `window([1, 2, 3, 4, 5], size=3, shift=1, stride=1)` --> `[[1, 2, 3], [2, 3, 4], [3, 4, 5]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1db743d6-e7a3-441b-aa3e-36b6ac614a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[1, 2, 3]\n",
      "[2, 3, 4]\n",
      "[3, 4, 5]\n",
      "[4, 5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 10:51:07.694391: W tensorflow/core/framework/dataset.cc:956] Input of Window will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(7).window(3, shift=1, drop_remainder=True, stride=1)\n",
    "\n",
    "for window in dataset:\n",
    "    print(list(window.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75342a-455c-407b-a7ff-107dbb901272",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.4. Understand Mean Absolute Error (MAE) and how it can be used to evaluate accuracy of sequence models.\n",
    "\n",
    "MAE is calculated by taking the absolute difference between the predicted and actual values ​​and averaging them. [10]\n",
    "\n",
    "* **Advantages** MAE is simple and easy to interpret as the mean error is expressed in the same units as the original data. It is less sensitive to outliers compared to other error metrics such as mean squared error (MSE).\n",
    "* **Disadvantages** MAE does not distinguish between overestimation and underestimation and does not provide information about the direction or magnitude of individual errors. In addition, depending on your particular problem it may be seen as a disadvantage that it does not penalize wrong predictions as much as MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "487bff0c-e79e-44cc-a58f-18e4e61eebb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.metrics.mae([1, 2, 3, 4], [2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9544d1-9ec0-4e4a-9a74-645e8db0f46c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.5. Use RNNs and CNNs for time series, sequence and forecasting models.\n",
    "\n",
    "* [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): Long Short Term Memory RNN [8]\n",
    "* [GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU): Gated Recurrent Unit RNN. The purpose of this architecture is to solve the vanishing gradient problem. [9]\n",
    "* [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional): Layer that process the sequence input in both directions using the corresponding RNN layer as a parameter (LSTM, GRU, etc.). [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5fab14-2f10-4439-bd1e-df77eec7ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LSTM Layer\n",
    "single_rnn_layer = tf.keras.layers.LSTM(12)\n",
    "stacked_rnn_layer = tf.keras.layers.LSTM(12, return_sequences=True)\n",
    "\n",
    "gru_layer = tf.keras.layers.GRU(12)\n",
    "\n",
    "bidirectional_lstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(12))\n",
    "stacked_bidirectional_lstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(12, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "baff9594-edd3-4db6-81c0-4f0673f8fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv Layers\n",
    "conv_layer = tf.keras.layers.Conv1D(filters=16, kernel_size=3)\n",
    "pooling_layer = tf.keras.layers.GlobalMaxPooling1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f0f47-3302-41b7-b925-c6c8c1f54e5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.6. Identify when to use trailing versus centered windows.\n",
    "\n",
    "* **Centered Moving Average** looks at the average around the given time, and of such will require knowledge of future values.\n",
    "    * center_ma(t) = mean(obs(t-1), obs(t), obs(t+1))\n",
    "    * A center moving average can be used as a general method to remove trend and seasonal components from a time series, a method that we often cannot use when forecasting.\n",
    "* **Trailing Moving Average** only uses historical observations and is used on time series forecasting.\n",
    "    * trail_ma(t) = mean(obs(t-2), obs(t-1), obs(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e87a9ea-595f-4935-9d82-8495f6543436",
   "metadata": {},
   "source": [
    "#### 5.7. Use TensorFlow for forecasting.\n",
    "\n",
    "Forecasting is the process of using the predicted value to forecast the next value for a given horizon. \n",
    "\n",
    "* NOTE: I have a custom utility to help with this in my `py-learning-toolbox` repo, `ml_toolbox.preprocessing.timeseries.make_future_forecasts` (I plan to move this later, but for now, it will stay here until I finish taking the exam)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228a61db-9da4-4ea6-869d-e98f841ca110",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.8. Prepare features and labels.\n",
    "\n",
    "Windowing is the main aspect of time series analysis. To extract features and labels for a deep learning model with windowed data, you need to break up the window and horizon.\n",
    "* Window: the number of features used\n",
    "* Horizon: the number of values to predict into the future\n",
    "\n",
    "```\n",
    "[0,1,2,3] --> [0,1,2] [3]\n",
    "[1,2,3,4] --> [1,2,3] [4]\n",
    "[2,3,4,5] --> [2,3,4] [5]\n",
    "[3,4,5,6] --> [3,4,5] [6]\n",
    "```\n",
    "\n",
    "&#128273; **NOTE**: I have a function in my `py-deep-learning` repository that will sequence, window, and split data into features and labels as shown in the example above\n",
    "* `ml_toolbox.preprocessing.timeseries.make_windowed_dataset`\n",
    "* **[Alternatively]** `ml_toolbox.preprocessing.timeseries.make_windows` with `ml_toolbox.preprocessing.timeseries.get_labels`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca4daa1-1991-470a-8d61-03f858774d08",
   "metadata": {},
   "source": [
    "#### 5.9. Identify and compensate for sequence bias.\n",
    "\n",
    "Sequence bias is when the order of things can impact the selection of things. \n",
    "\n",
    "TODO: Need to figure this out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb272cf5-b6bd-4dd9-ba33-0a09e4083707",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.10. Adjust the learning rate dynamically in time series, sequence and prediction models.\n",
    "\n",
    "This can be done using the learning rate callbacks as mentioned in section 2.16.\n",
    "\n",
    "* `ReduceLROnPlateau` callback\n",
    "* `LearningRateScheduler` callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f3b260d-d0e2-4b7a-9aa7-4998e94d3d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduces the learning rate by a factor when the learning stops (hits a plateau)\n",
    "lr_reducer = tf.keras.callbacks.ReduceLROnPlateau()\n",
    "\n",
    "# LR Scheduler is used to update the learning rate for each epoch\n",
    "# This is useful when triying to determine the optimal learning rate\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch, lr: lr * 10 ** 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
