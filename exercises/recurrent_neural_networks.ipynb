{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151f8d62-135c-4f43-b4d9-23b0f478bd55",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Natural language processing (NLP) is the practice of identify sequence patterns from language, to deduce the meaning behind the statement. In short, NLP has the goal of derviging information out of natural language (could be sequences text or speech). Another common term for NLP problems is sequence to sequence problems (seq2seq).\n",
    "\n",
    "The purpose of this notebook is to download, prepare, and use a text dataset to build out multiple recurrent neural network (RNN) models to make predictions from the text. Additionally, I will create a model from an already pre-trained model on TensorFlow Hub.\n",
    "\n",
    "The dataset I am going to use is Kaggle's introduction to NLP dataset (text samples of Tweets that predict as disaster or not disater).\n",
    "* https://www.kaggle.com/competitions/nlp-getting-started\n",
    "\n",
    "NOTE: Other sequence problems may include something like time series forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c742ac89-6eb4-408b-bb8c-e7714e5ee8ae",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5911a4-5d95-4c3c-b705-2ab68389eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "import io\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from py_learning_toolbox import dl_toolbox\n",
    "from py_learning_toolbox import performance_toolbox\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49152c-d630-403b-b359-8fe3342a2566",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ee5bf-7763-4ab9-a993-e9a09fd773aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "757fb2a1-34f8-42ae-9b66-49402497eea3",
   "metadata": {},
   "source": [
    "## Download and Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55983e1-a4d0-4784-bd87-368837f4f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dataset location\n",
    "data_directory = pathlib.Path('./data/nlp_getting_started')\n",
    "test_file = data_directory / 'test.csv'\n",
    "train_file = data_directory / 'train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b0a6c-883b-4418-91ca-eca26676223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the text dataset\n",
    "train_data = pd.read_csv(str(train_file))\n",
    "test_data = pd.read_csv(str(test_file))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b9ef0-d397-4eb7-9576-4c2d082d6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets shuffle the training data\n",
    "train_data_shuffled = train_data.sample(frac=1, random_state=42)\n",
    "train_data_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55002b83-a025-4c04-b7ee-cd0d07e6886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at 10 random tweets and whether each one was a disaster or not\n",
    "for i in range(10):\n",
    "    row = train_data_shuffled.iloc[random.randint(0, len(train_data_shuffled))]\n",
    "    print(f\"Target: ({'Disaster' if row['target'] else 'Not Disaster'})\")\n",
    "    print(row['text'])\n",
    "    print('\\n', '-' * 40, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe153d8-0b12-4639-83e1-ea1d76ba1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the number of each target (disaster or not a disaster)\n",
    "train_data_shuffled.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e40ffb-9ff1-4e5e-a4d0-de7acd2080d6",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "To prep this data, there are a few things I need to do to get everything ready to build out my models.\n",
    "\n",
    "1. Shuffle the training data set.\n",
    "2. Split the training data set into a training and validation set (go to use 10% of the training data as the validation data).\n",
    "3. Need to convert text into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6032c2-ffa7-433c-98ef-2dbcaf008cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets shuffle the training data\n",
    "train_data_shuffled = train_data.sample(frac=1, random_state=42)\n",
    "train_data_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e54c759-ffbd-45b6-9e14-3a9782102593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the train data to split into training and validation datasets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    train_data_shuffled['text'].to_numpy(),\n",
    "    train_data_shuffled['target'].to_numpy(),\n",
    "    test_size=0.1,\n",
    "    random_state=42)\n",
    "\n",
    "len(train_sentences), len(val_sentences), len(train_labels), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3d60b-2d08-42bc-9dd6-86a7e1da5715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the split worked as expected\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f52f52-64a8-4ab2-a248-9579817be75f",
   "metadata": {},
   "source": [
    "#### TextVectorization Layer (To be Used in Models)\n",
    "\n",
    "When dealing with a text problem, one of the first things to do before building a model is to convert text to numbers. There are a few ways to do this:\n",
    "\n",
    "* Tokenization - direct mapping of token (a token could be a word or a character) to a number.\n",
    "* Embedding - create a matrix of featyre vector for each token (the size of the feature vector can be defined and this embedding can be learned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a65e4-0586-4e3a-b607-8519af6288f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find average number of tokens\n",
    "round(sum([len(i.split()) for i in train_sentences]) / len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f1eeb-d155-493b-87d9-7219f96e16ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup text vectorization params\n",
    "max_vocab_length = 10000  # Max words to have in our vocab\n",
    "max_length = 15  # Max length our sequence will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac6e7dc-e2de-470c-8fd6-f733cf3458dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a text vectorization layer (tokenization)\n",
    "text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=max_vocab_length,  # How many words in the vocabulary (None sets as no maximum number of tokens)\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_length)  # Padds (adds 0's to end of number) to make all the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2bd6e-93af-47ab-b531-5041edd1e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt the vectorizer to the training data\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b4422-e3ed-4bb6-9ce6-b358b667caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the text vectorizer was adapted correctly\n",
    "sample_sentence = 'There\\'s a flood in my street!'\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53f63e-0ef4-45e1-91c3-bdf425681868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random sentence from the train data and encode it\n",
    "rand_i = random.randint(0, len(train_sentences))\n",
    "print(f'Sentence: {train_sentences[rand_i]}')\n",
    "print(f'Vectorized: {text_vectorizer([train_sentences[rand_i]])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a91fb2-40d4-4fc3-94bf-b62e64e3bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the words in the vocab from the training data\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "least_common_5_words = words_in_vocab[-5:]\n",
    "len(words_in_vocab), top_5_words, least_common_5_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56630459-a8a1-44a5-a2b1-9444a02170d7",
   "metadata": {},
   "source": [
    "#### Creating Embedding Layer (To be Used in Models)\n",
    "\n",
    "To make our embedding layer, I am going to use TensorFlow's `Embedding` layer. \n",
    "\n",
    "The parameters we care most about for our embedding layer are:\n",
    "\n",
    "* `input dim` -  The size of the vocabulary\n",
    "* `output dim` - The size of the output embedding vector, for example, a size of 100 mean each token would be represented by a vector of length 100.\n",
    "* `input_length` - The length of the sequences being passed to the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfcee59-812a-4c26-a072-27b9f9c8d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,  # GPU's work well when number is divisible by 8\n",
    "                                     input_length=max_length)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb8b447-6d24-4c6e-8236-2ae64e9f8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the embedding layer worked\n",
    "random_sentence = random.choice(train_sentences)\n",
    "embedded_sentence = embedding(text_vectorizer([random_sentence]))\n",
    "print('Sentence: \\n', random_sentence)\n",
    "print('Embedded Version: \\n', embedded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f31d78-10d5-4fa6-9019-17398bf1111b",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "To experiment and identify the best model, I am going to run the following experiments with the corresponding model to analyze the difference between the different types of models for modeling sequence based problems.\n",
    "\n",
    "1. Naive Bayes with TF-IDF encoder (baseline model) NOTE: this is not a Deep Learning model\n",
    "2. Feed-forward neural network (dense model)\n",
    "3. LSTM (RNN)\n",
    "4. GRU (RNN)\n",
    "5. Bidirection-LSTM (RNN)\n",
    "6. 1D Convolutional Neural Network\n",
    "7. TensorFlow Hub Pretrained Feature Extractor\n",
    "8. TensorFlow Hub Pretrained Feature Extractor (10% of Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05d22a-55ff-4b9f-81c4-21f74fd36a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "TENSORBOARD_LOGS_DIR = pathlib.Path('logs/disaster_tweets')\n",
    "CHECKPOINTS_DIR = pathlib.Path('checkpoints/disaster_tweets')\n",
    "MODELS_DIR = pathlib.Path('models/disaster_tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1862f-9330-4fd5-b3d9-ead3f585c8d9",
   "metadata": {},
   "source": [
    "### Model-0 (Baseline Model): Naive Bayes Model\n",
    "\n",
    "As a baseline model, I am going to use SKLearn's Multinomial Naive Bayes algorithm using the TF-IDF formuila to convert words to numbers. This model will be used to compare the DL models against to judge performance.\n",
    "\n",
    "NOTE: It's common practice to use non-LD algorithms as a baseline because of their speed and then later using DL to see if you can improve upon them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96442451-4399-44f0-b12d-a4ef31d77f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building out the baseline model\n",
    "\n",
    "# Build Model\n",
    "model_0 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Fit Model\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20451e2-c1b3-4d3e-8e9c-80825b88793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model (SKlearn uses accuracy as the metric)\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "baseline_score  # Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b6e0b-829c-4e03-ba51-a1cffa9415dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9f5f5-d892-4ef6-a304-8e0297fc8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = dl_toolbox.analysis.classification.generate_prediction_metrics(val_labels, baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39338d8-68c6-435d-aa7c-7c2b196b6de3",
   "metadata": {},
   "source": [
    "### Model-1: Feed Forward Dense Model\n",
    "\n",
    "The first test I am going to run against my baseline model is to use the traditional Dense DL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbfad7-df0b-4691-850b-1cad30ef8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string) # Inputs are 1 dimensional strings\n",
    "x = text_vectorizer(inputs)  # Turn the input text into numbers\n",
    "x = embedding(x)  # Embed the text\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)  # Create the output layer\n",
    "\n",
    "model_1 = tf.keras.models.Model(inputs, outputs, name='DenseModel')\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67ff4b-50cf-432e-a11f-709b9766db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48076b-8252-4e39-9427-535ba6ebd086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Callbacks\n",
    "tensorboard_callback_1 = dl_toolbox.modeling.callbacks.generate_tensorboard_callback('dense-model', str(TENSORBOARD_LOGS_DIR))\n",
    "\n",
    "# Fit the model\n",
    "model_1_history = model_1.fit(x=train_sentences,\n",
    "            y=train_labels,\n",
    "            epochs=5,\n",
    "            validation_data=(val_sentences, val_labels),\n",
    "            callbacks=[tensorboard_callback_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed8bf60-9d38-41fd-acf3-49e2654f8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f72702-211e-44d0-b2cb-01d7ae0caa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f92c99-e8c0-48eb-b2db-b6e011fb23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prediction probabilities to 1 or 0\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa9630-6cbc-48f9-8139-907a505b5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_results = dl_toolbox.analysis.classification.generate_prediction_metrics(val_labels, model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d75b29-f373-41b5-b5c3-d84598ad92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data against the baseline data\n",
    "np.array(list(dict(model_1_results).values())) >= np.array(list(dict(baseline_results).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d054d93-bf80-42b7-9a9a-3a335dd20784",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "Looks like the baseline outperformed the simple Dense DL model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca71a6-cc7c-4956-8074-1a4519ae6732",
   "metadata": {},
   "source": [
    "#### Visualizing the Learned Embeddings\n",
    "\n",
    "To visualize the embedding matrix, TensorFlow has a handy tool called projector that visualizes the matrix.\n",
    "\n",
    "NOTE: To utilize the projector tool, you need to create a vectors.tsv and metadata.tsv that will be uploaded to the projector website linked below.\n",
    "\n",
    "* https://www.tensorflow.org/text/guide/word_embeddings\n",
    "* https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b7d1a8-c022-4240-b849-5fdf3771f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weight matrix of embedding layer\n",
    "# These are the numerical representation of each token in our training data, learned for 5 epochs.\n",
    "embed_weights_1 = model_1.get_layer('embedding').get_weights()[0]\n",
    "embed_weights_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e940973-74b3-424b-bbf0-438d0678daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the shape, the embedding matrix is 10,000 x 128 matrix\n",
    "# (every token in vocabulary has 128 params to better represent each token)\n",
    "embed_weights_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e878b9e-d8d2-424d-8522-4ef0314ecc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding files (These will be uploaded to the embedding projector)\n",
    "filepath = f'{str(TENSORBOARD_LOGS_DIR)}/dense-model/embedding_projector'\n",
    "dl_toolbox.analysis.export.export_embedding_projector_data(embed_weights_1, words_in_vocab, filepath, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7054a5-4c7d-4fda-a872-c003aa674369",
   "metadata": {},
   "source": [
    "### Model-2: LSTM\n",
    "\n",
    "LSTM (Long Short Term Memory) is one of the most popular RNN models.\n",
    "\n",
    "Our structure of an RNN typically looks like this:\n",
    "\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers (RNN's/Dense) -> Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388e221-2f44-4ad0-bc11-41e389ba7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LSTM Model\n",
    "\n",
    "# Build model\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string) # Inputs are 1 dimensional strings\n",
    "\n",
    "x = text_vectorizer(inputs)  # Turn the input text into numbers\n",
    "x = embedding(x)  # Embed the text\n",
    "# x = tf.keras.layers.LSTM(64, return_sequences=True)(x)  # when you're stacking RNN cells together, you need to set return sequences to True\n",
    "x = tf.keras.layers.LSTM(64)(x)\n",
    "# x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_2 = tf.keras.models.Model(inputs, outputs, name='Model2LSTM')\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0829d5-5abd-4559-b7b7-4f6beaea6b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_2.compile(loss='binary_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827fd22-4c30-483e-911a-fdcce2878a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Callbacks\n",
    "tensorboard_callback_2 = dl_toolbox.modeling.callbacks.generate_tensorboard_callback('lstm-model', str(TENSORBOARD_LOGS_DIR))\n",
    "\n",
    "# Fit the Model\n",
    "model_2_history = model_2.fit(train_sentences,\n",
    "            train_labels,\n",
    "            epochs=5,\n",
    "            validation_data=(val_sentences, val_labels),\n",
    "            callbacks=[tensorboard_callback_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc7f09-a0d1-4e38-98f2-1535db614631",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_toolbox.analysis.history.plot_history(model_2_history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043b6ce-a9e1-4281-bf68-a2793caea3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae89a95b-d25a-46ef-a7ce-e2cd6a5c732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc23833-2b7d-414e-b857-5ff4297fea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_results = dl_toolbox.analysis.classification.generate_prediction_metrics(val_labels, model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff0c66-daaf-45f2-b34d-f4f1445b4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data against the baseline data\n",
    "np.array(list(dict(model_1_results).values())) >= np.array(list(dict(baseline_results).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06ef56-f224-4fde-806e-6e5c3aaaab42",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "Looks like the baseline model is still outperforming the LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be6ac4-260f-4dc9-b6b5-270446df1aa2",
   "metadata": {},
   "source": [
    "### Model-3: GRU\n",
    "\n",
    "Another popular and effective RNN component is the FRU or gated recurrent unit. The GRU cell has similar features to an LSTM cell, but has less parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e542694-dc83-4390-a639-209450496c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GRU model\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.GRU(64)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_3 = tf.keras.models.Model(inputs, outputs, name='Model3GRU')\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af53a9-d9ed-4864-9326-c40fab38899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "model_3.compile(loss='binary_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1053b-3660-4568-b881-1847864bc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with Tensorboard callback\n",
    "# Setup Callbacks\n",
    "tensorboard_callback_3 = dl_toolbox.modeling.callbacks.generate_tensorboard_callback('gru-model', str(TENSORBOARD_LOGS_DIR))\n",
    "\n",
    "# Fit Model\n",
    "model_3_history = model_3.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[tensorboard_callback_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803dde71-9ffb-4597-be5b-1ec36d888a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_toolbox.analysis.history.plot_history(model_3_history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0296de-ab8f-4bc1-bfe1-c7a7e2800daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba14b6-b464-4c93-833d-2748d44156d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effff607-897b-400d-a592-5d5b6b9e5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_results = dl_toolbox.analysis.classification.generate_prediction_metrics(val_labels, model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75385c70-3992-4e86-b4c7-0c9ebb64cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(dict(model_3_results).values())) >= np.array(list(dict(baseline_results).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8cf9f9-d855-44c5-879d-0ebe3af91c30",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "Still haven't beat the baseline model :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e15cd40-6ca5-4bf0-8688-bc37fe59a895",
   "metadata": {},
   "source": [
    "### Model-4: Bidirectional LSTM Model\n",
    "\n",
    "Normal RNN's go from left to right, however, Bidirectional RNN's go from left to right as well as right to left. To summarize, it reads a sentence from left to right, then reads it from right to left.\n",
    "\n",
    "NOTE: These are really only useful when going both directions can teach the network something useful when going both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2db689-84b2-4d69-8a8f-460f5bb28c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build out the Bidirectional Model\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_4 = tf.keras.models.Model(inputs, outputs, name='Model4BidirectionalLSTM')\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ccfc7-6459-4ab4-ac80-8b84a8c547da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model_4.compile(loss='binary_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e16408-f09f-4124-880b-58f4314c937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback and Fit\n",
    "# Fit the model with Tensorboard callback\n",
    "# Setup Callbacks\n",
    "tensorboard_callback_4 = dl_toolbox.modeling.callbacks.generate_tensorboard_callback('bidirectional-lstm-model', str(TENSORBOARD_LOGS_DIR))\n",
    "\n",
    "model_4_history = model_4.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[tensorboard_callback_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37885c88-dc5a-418f-8f0e-bc99546432c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_toolbox.analysis.history.plot_history(model_4_history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08667d28-4bc2-4ce9-913b-eb8fb6b5e9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4_pred_probs = model_4.predict(val_sentences)\n",
    "model_4_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b08dc-957e-4edb-98a1-89d70bac896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a758d3d-b933-41de-b069-6a15a5c99b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4_results = dl_toolbox.analysis.classification.generate_prediction_metrics(val_labels, model_4_preds)\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e43681-ed27-4288-947b-89649145fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(dict(model_4_results).values())) >= np.array(list(dict(baseline_results).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f41862-200d-406d-bbd6-9bfcaf7fe991",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "Looks like this performed worse than the LSTM and GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9fa2e7-34cc-4181-a27e-53ffccb6dc98",
   "metadata": {},
   "source": [
    "### Model-5: 1D Convolutional Neural Network\n",
    "\n",
    "We've used CNN's for images, but images are tpycally 2D, however, text data is 1D. \n",
    "\n",
    "The typical structure for Conv1D models:\n",
    "\n",
    "```\n",
    "Inputs -> Tokenization -> Embedding -> Layers (Conv1D + Pooling) -> Outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f049a-767b-40c2-9a24-d38df107937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation='relu', padding='valid')(x)\n",
    "x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_5 = tf.keras.models.Model(inputs, outputs, name='Model5CNN1D')\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c61704-b16d-4b26-8a1a-01bc79b4c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_5.compile(loss='binary_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e51d4e-b315-4fbd-a352-593bf4254879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback and Fit\n",
    "# Fit the model with Tensorboard callback\n",
    "# Setup Callbacks\n",
    "tensorboard_callback_5 = dl_toolbox.modeling.callbacks.generate_tensorboard_callback('conv-1d-model', str(TENSORBOARD_LOGS_DIR))\n",
    "\n",
    "model_5_history = model_5.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[tensorboard_callback_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2073372-d8f2-41a9-814a-6e41a895273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_toolbox.analysis.history.plot_history(model_5_history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804c3b6-f269-456a-a6d0-f9f68580e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5_pred_probs = model_5.predict(val_sentences)\n",
    "model_5_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce4ddd-c8b0-478b-af9d-5df70eff1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931b993-7baa-42bd-9239-5c4a3f25c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5_results = dl_toolbox.analysis.classification.generate_prediction_metrics(val_labels, model_5_preds)\n",
    "model_5_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9df2c2-6068-4157-b276-5baac48c9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(dict(model_5_results).values())) >= np.array(list(dict(baseline_results).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b209cd-6336-4d28-9745-2d179d4740fe",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "Still not outperforming our Baseline Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d302c-4de2-4d1e-b271-d89da0f505bd",
   "metadata": {},
   "source": [
    "### Model-6: TensorFlow Hub Pretrained Sentence Encoder\n",
    "\n",
    "This model will use Transfer Learning with the `Universal Sentence Encoder` pretrained model on TensorFlow Hub (see link below).\n",
    "\n",
    "* https://tfhub.dev/google/collections/universal-sentence-encoder/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fe3aa-3d62-4f24-813f-afadd1ad3442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f76d64-f239-4433-999b-0b29aad5c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_url = 'https://tfhub.dev/google/universal-sentence-encoder/4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac632d8-44d7-4528-b6ff-4f6ba86dde13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out the transfer learning model\n",
    "embed = hub.load(use_url)\n",
    "\n",
    "embeddings = embed([\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I am a sentence for which I would like to get its embedding\"])\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ee9fb-55d1-4065-b732-e5c105d984cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "\n",
    "# Create a Keras Layer using the USE pretrained layer from TensorFlow Hub\n",
    "sentence_encoder_layer = hub.KerasLayer(use_url, input_shape=[], dtype=tf.string, trainable=False, name='USE')\n",
    "\n",
    "# Setup Layers\n",
    "model_6 = tf.keras.models.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')], name='Model6USE')\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3916d58-a751-4339-9f77-011129fad23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_6.compile(loss='binary_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70635e-a5dd-4a58-a47a-27e90443bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback and Fit\n",
    "# Fit the model with Tensorboard callback\n",
    "# Setup Callbacks\n",
    "tensorboard_callback_6 = dl_toolbox.modeling.callbacks.generate_tensorboard_callback('use-model', str(TENSORBOARD_LOGS_DIR))\n",
    "\n",
    "model_6_history = model_6.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[tensorboard_callback_6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee1fa1b-b45b-492c-8abf-597a684b5968",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_toolbox.analysis.history.plot_history(model_6_history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc095472-7ccd-40a6-8a85-68390860efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6_pred_probs = model_6.predict(val_sentences)\n",
    "model_6_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3ea00-782f-483a-8516-8c1dad2e84ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6_preds = tf.squeeze(tf.round(model_6_pred_probs))\n",
    "model_6_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a9e001-8614-4145-a953-17a743a35e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6_results = dl_toolbox.analysis.classification.generate_prediction_metrics(val_labels, model_6_preds)\n",
    "model_6_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1469b-1e24-4662-9541-f05e6af1a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(dict(model_6_results).values())) >= np.array(list(dict(baseline_results).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31b2b6-32f3-43fb-8ff4-5cc7c093fe94",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "Looks like this model beat the baseline the first time I ran this, but it was very close and isn't guranteed to beat it every time due to randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e802955-7661-4d08-a226-a9f04507a71e",
   "metadata": {},
   "source": [
    "### Model-7: TF Hub Pretrained USE but w/ 10% of Training Data\n",
    "\n",
    "Transfer learning helps when you don't have a large dataset. To see how our model performs on a smaller dataset, I am going to replicate model 6, but I will only train it on 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c138a9-739b-4156-ab3b-a5c8553415ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 10% subset of the training data\n",
    "train_10_percent_split = int(0.1 * len(train_sentences))\n",
    "train_sentences_10_percent = train_sentences[:train_10_percent_split]\n",
    "train_labels_10_percent = train_labels[:train_10_percent_split]\n",
    "len(train_sentences_10_percent), len(train_labels_10_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc98df-67e8-4c26-be18-1c2c7ccdd094",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "When looking at the 10% sample, needed to verify that the subset is representative of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56990c5-41e7-4b4f-993c-18bc3691c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array(train_labels_10_percent)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ff624-b44a-467c-bced-8be81bca2418",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_shuffled['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef01066-c267-4534-9937-cfadb4dad604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model_7 = tf.keras.models.clone_model(model_6)\n",
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceaa15c-bdd6-4aa8-a210-a7be5a1a37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_7.compile(loss='binary_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b2f50-3c67-473c-ae24-e70c2d9dfdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback and Fit\n",
    "# Fit the model with Tensorboard callback\n",
    "# Setup Callbacks\n",
    "tensorboard_callback_7 = dl_toolbox.modeling.callbacks.generate_tensorboard_callback('use-10-percent-model', str(TENSORBOARD_LOGS_DIR))\n",
    "\n",
    "model_7_history = model_7.fit(train_sentences_10_percent,\n",
    "                              train_labels_10_percent,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[tensorboard_callback_7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306be35-b855-430a-9637-bc6977d65c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_toolbox.analysis.history.plot_history(model_7_history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ff20a-a825-443e-a2a4-e559eee6959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7_pred_probs = model_7.predict(val_sentences)\n",
    "model_7_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55f491-d784-4d44-92ea-1952dc112ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
    "model_7_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd77be1-13a7-4fce-8052-8271f21b141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7_results = dl_toolbox.analysis.classification.generate_prediction_metrics(val_labels, model_7_preds)\n",
    "model_7_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c6627-4c66-4f48-af76-5a6861d1dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(dict(model_7_results).values())) >= np.array(list(dict(baseline_results).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac083b5e-d8ab-4284-b44d-cf110485d908",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "Even with only 10% of the data, it performed only slightly worse than when training the model on 100% of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d40072-35cc-4e78-b789-4a103921059f",
   "metadata": {},
   "source": [
    "## Comparing the Performance of Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc44e1-da95-4bcc-9bac-452d502ed318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the combined performance of each model\n",
    "all_model_results = pd.DataFrame({\n",
    "    '0_baseline': dict(baseline_results),\n",
    "    '1_simple_dense': dict(model_1_results),\n",
    "    '2_lstm': dict(model_2_results),\n",
    "    '3_gru': dict(model_3_results),\n",
    "    '4_bidirectional': dict(model_4_results),\n",
    "    '5_conv1d': dict(model_5_results),\n",
    "    '6_tf_hub_use_encoder': dict(model_6_results),\n",
    "    '7_tf_hub_use_encoder_10_percent': dict(model_7_results),\n",
    "})\n",
    "all_model_results = all_model_results.transpose()\n",
    "all_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0af41-1905-4005-b27d-95e094904091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and compare all of the model results\n",
    "all_model_results.plot(kind='bar', figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610fa8b6-7f6e-42ee-97c5-ba91a93e5692",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_results.sort_values('f1', ascending=False)['f1'].plot(kind='bar', figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70239b9d-d4f9-4046-adfe-262be33f4d53",
   "metadata": {},
   "source": [
    "### Finding Most Wrong Examples\n",
    "\n",
    "* If our best model still isn't perfect, what examples is it getting wrong?\n",
    "* And of these wrong examples, which ones is it getting *most* wrong.\n",
    "\n",
    "For example if a sample should have a label of 0, but our model predicts a prediction probability of 0.999, that is pretty wrong.\n",
    "\n",
    "To do this, I am going to look at Model 6 because that model performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3d86a-f79c-437b-be0e-dd1b80320d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6_pred_probs[:10], model_6_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1274d4e7-646c-493a-aa87-b976b88c78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CReateing DataFram with data\n",
    "val_df = pd.DataFrame({\n",
    "    'text': val_sentences,\n",
    "    'target': val_labels,\n",
    "    'pred': model_6_preds,\n",
    "    'pred_prob': tf.squeeze(model_6_pred_probs)\n",
    "})\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac3609-6c83-4bcb-adeb-24717ddd7492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find wrong predictions and sort by prediction probs\n",
    "most_wrong = val_df[val_df['target'] != val_df['pred']].sort_values('pred_prob', ascending=False)\n",
    "most_wrong[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c14948-9146-4e47-a999-592af273a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wrong[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83248740-bbe2-4221-80f9-35ee26f15914",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in most_wrong[-10:].itertuples():\n",
    "    _, text, target, pred, pred_prob = row\n",
    "    print(f'Target: {target}, Pred: {pred}, Prob: {pred_prob}')\n",
    "    print('Text: ', text)\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1174c26-cc49-4562-9832-aeddf59bc60c",
   "metadata": {},
   "source": [
    "## Making Predictios on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6e67b-9f31-445f-8670-cd32c6e229bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113a105c-76bc-4511-b90b-9ff74ed9cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_data['text'].to_list()\n",
    "test_samples = random.sample(test_sentences, 10)\n",
    "\n",
    "for test_sample in test_samples:\n",
    "    pred_prob = tf.squeeze(model_6.predict([test_sample]))\n",
    "    pred = tf.round(pred_prob)\n",
    "\n",
    "    print(f'Pred: {int(pred)}, Prob: {pred_prob}')\n",
    "    print('Text: ', test_sample)\n",
    "    print('-' * 80, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57bb11-7bd8-4ee0-805b-13a9939a1d11",
   "metadata": {},
   "source": [
    "## Speed vs. Score Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff8af5-ae18-4adc-bc40-b4435c4c64db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6_performance = performance_toolbox.model.prediction_timer(model_6, val_sentences)\n",
    "model_6_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af77e9-702e-4f9a-9888-5a19e0cc6f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_performance = performance_toolbox.model.prediction_timer(model_0, val_sentences)\n",
    "model_0_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9e087-cbb8-4cf0-b1b5-40ff670baada",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(model_0_performance.time_per_prediction, baseline_results.f1, label='baseline')\n",
    "plt.scatter(model_6_performance.time_per_prediction, model_6_results.f1, label='model 6')\n",
    "plt.legend()\n",
    "plt.title('F1-score vs. Time per prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c38296-c67c-4d9c-beca-2171cad85210",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "When comparing out best model against our baseline model, the time difference is significant, even though the `F1` is virtually identical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
