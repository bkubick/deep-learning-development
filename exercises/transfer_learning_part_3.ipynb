{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e24d1d7a-14d4-48b4-970f-c8c069c102c8",
   "metadata": {},
   "source": [
    "# Transfer Learning - Part 3 (Food Vision Mini)\n",
    "## Scaling Up\n",
    "\n",
    "The previous two notebooks that I am digging into Transfer Learning has been done using only 10 food vision classes, but there are 101 classes in the actual dataset. The purpose of this notebook is to scale up and look at a larger model.\n",
    "\n",
    "Our goal is to beat the original Food 101 paper with only 10% of the training data. The paper accuracy was 50.76% when training on 101,000 images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db375d-603f-44ce-9fbe-f16443c281ad",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69181dc-672c-4d27-b148-3e974f7a6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "from typing import Tuple\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e9ad2e-e33b-4115-b468-c0fd92ecc5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff56b5-7a3b-4702-ab20-3d492297918a",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fed09-f053-483d-b6c4-912b74495e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_num_trainable_layers(model):\n",
    "    print('Total Trainable Variables: ', len(model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755160a-4399-417c-bad2-84f848b07e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_checkpoint_callback(checkpoint_path: str, best_only: bool = True) -> tf.keras.callbacks.ModelCheckpoint:\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_accuracy',\n",
    "        save_weights_only=True,\n",
    "        save_best_only=best_only,\n",
    "        save_freq='epoch',\n",
    "        verbose=1)\n",
    "\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def generate_csv_logger_callback(filename: str) -> tf.keras.callbacks.CSVLogger:\n",
    "    logger = tf.keras.callbacks.CSVLogger(f'logs/csv/{filename}')\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ed0d6-bc02-465b-8d58-5c54be22104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_histories(original_history, new_history, initial_epoch):\n",
    "    if isinstance(original_history, pd.DataFrame) and isinstance(new_history, pd.DataFrame):\n",
    "        original_history_df = original_history\n",
    "        new_history_df = new_history\n",
    "    else:\n",
    "        original_history_df = pd.DataFrame(original_history.history)\n",
    "        new_history_df = pd.DataFrame(new_history.history)\n",
    "\n",
    "    total_acc = pd.concat([original_history_df['accuracy'], new_history_df['accuracy']])\n",
    "    total_loss = pd.concat([original_history_df['loss'], new_history_df['loss']])\n",
    "    total_val_acc = pd.concat([original_history_df['val_accuracy'], new_history_df['val_accuracy']])\n",
    "    total_val_loss = pd.concat([original_history_df['val_loss'], new_history_df['val_loss']])\n",
    "    \n",
    "    # Loss Plots\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(total_loss, label='Training Loss')\n",
    "    plt.plot(total_val_loss, label='Validation Loss')\n",
    "    plt.plot([initial_epoch-1, initial_epoch-1], plt.ylim(), label='Start Fine Tuning')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    # Accuracy Plots\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(total_acc, label='Training Accuracy')\n",
    "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
    "    plt.plot([initial_epoch-1, initial_epoch-1], plt.ylim(), label='Start Fine Tuning')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bdce03-c1d3-4403-91f8-73458df2ccbc",
   "metadata": {},
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a843db-c4eb-4fcc-a128-15928529d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = pathlib.Path('./data/food-101/101_food_classes_10_percent/')\n",
    "train_directory = data_directory / 'train'\n",
    "test_directory = data_directory / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e8117-1cd3-45fa-85e3-8f8c59b9480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.image.summarize_image_directory(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e3eec8-c555-4733-b3f8-5a513cf554c5",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "After looking at the directory, looks like there are 75 training images for each category of food, and 250 test images for each category of food."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e645b7-b916-4ee9-b9d8-c756b5b86af8",
   "metadata": {},
   "source": [
    "## Data Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde003a9-edec-4e8c-ad48-a0d6dd201607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b3787-20e8-46a5-9a8c-031d0def7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.keras.utils.image_dataset_from_directory(str(train_directory),\n",
    "                                                         image_size=(img_size, img_size),\n",
    "                                                         label_mode='categorical')\n",
    "\n",
    "test_data = tf.keras.utils.image_dataset_from_directory(str(test_directory),\n",
    "                                                        image_size=(img_size, img_size),\n",
    "                                                        shuffle=False,\n",
    "                                                        label_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4ab38-ce8f-4985-b93d-3e7945b13693",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_image_classes = len(train_data.class_names)\n",
    "num_image_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b16769b-ae34-4742-a070-1e37f92920aa",
   "metadata": {},
   "source": [
    "## Creating a Model\n",
    "\n",
    "Steps to creating the model to identify 101 food classes.\n",
    "\n",
    "1. Create a model checkpoint callback. (**UPDATE** I ended up adding two additional callbacks to better save checkpoints of the trained model)\n",
    "2. Create a data augmentation layer to build the data augmentation into the model directly.\n",
    "3. Build a headless (no top layers) functional EfficientNetB0 base model.\n",
    "4. Compile our Model.\n",
    "5. Feature extract for 5 epochs on training data set, and validate on 15% of the test data to save time per epoch.\n",
    "6. Adjust model and repeate (implement fine tuning, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eaf661-6de0-440e-8425-92d80f10067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_path = 'models/101_food_classes/model_0'\n",
    "\n",
    "try:\n",
    "    model_0 = tf.keras.models.load_model(model_0_path)\n",
    "    train_model_0: bool = False\n",
    "except OSError:\n",
    "    train_model_0: bool = True\n",
    "    \n",
    "print('Train Model 0: ', train_model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a3b19-647e-4701-806f-afe1b7de5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Callbacks\n",
    "best_weights_checkpoint_path_0 = 'checkpoints/101_classes_10_percent/model_0/best_epoch/checkpoint.ckpt'\n",
    "best_weights_checkpoint_callback_0 = generate_checkpoint_callback(best_weights_checkpoint_path_0)\n",
    "\n",
    "last_epoch_checkpoint_path_0 = 'checkpoints/101_classes_10_percent/model_0/last_epoch/checkpoint.ckpt'\n",
    "last_epoch_checkpoint_callback_0 = generate_checkpoint_callback(last_epoch_checkpoint_path_0, best_only=False)\n",
    "\n",
    "csv_logger_name_0 = '101_classes_10_percent/model_0.csv'\n",
    "csv_logger_callback_0 = generate_csv_logger_callback(csv_logger_name_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8865cf-19a8-4780-a4a1-0b2f456b4d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Augmentation Layer (Scaling layer not required because EfficientNetB0 already has that incorporated)\n",
    "data_augmentation = tf.keras.models.Sequential([\n",
    "    preprocessing.RandomFlip('horizontal'),\n",
    "    preprocessing.RandomRotation(0.2),\n",
    "    preprocessing.RandomHeight(0.2),\n",
    "    preprocessing.RandomWidth(0.2),\n",
    "    preprocessing.RandomZoom(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f1a700-f36f-46ed-8ade-b0312bfd6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Building out the Transfer Learning Model\n",
    "\n",
    "# EfficientNetB0 Base Model\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Input Layer\n",
    "inputs = layers.Input(shape=(img_size, img_size, 3), name='InputLayer')\n",
    "\n",
    "# Incorporate Data Augmentation Layer\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "# Incorporate Base Model\n",
    "x = base_model(x, training=False)\n",
    "\n",
    "# Incorporate the Global Average Pooling Layer\n",
    "x = layers.GlobalAveragePooling2D(name='GlobalAveragePoolingLayer')(x)\n",
    "\n",
    "# Incorporate Output Layer\n",
    "outputs = layers.Dense(num_image_classes, activation='softmax', name='OutputLayer')(x)\n",
    "\n",
    "# Build the model\n",
    "model_0 = tf.keras.models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32230d9-41f7-4a54-b9e3-26dbd94c45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d4bca-9a14-482d-9317-1d94df9624eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize.visualize_model(model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18f383-707f-4e08-898e-956faa4ba061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compile Model\n",
    "if train_model_0:\n",
    "    model_0.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4dfdf2-295b-4f53-abbd-06f1cf6ce7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch_0 = 0\n",
    "num_epochs_0 = 5\n",
    "percent_of_validation_data = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8bbdb-1d26-4a44-ae76-fe5722ad7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If fit was interupted before completing the entire 5 epochs of training, uncomment below and update the initial epoch.\n",
    "# initial_epoch_0 = 1\n",
    "# model_0.load_weights(last_epoch_checkpoint_path_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a0b7f1-d3e8-4ef5-a49b-08caa70fc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Fit Model on 5 epochs\n",
    "num_epochs_0 = 5\n",
    "percent_of_validation_data = 0.15\n",
    "\n",
    "if train_model_0:\n",
    "    history_0 = model_0.fit(train_data,\n",
    "                            epochs=num_epochs_0,\n",
    "                            initial_epoch=initial_epoch_0,\n",
    "                            steps_per_epoch=len(train_data),\n",
    "                            validation_data=test_data,\n",
    "                            validation_steps=int(percent_of_validation_data * len(test_data)),\n",
    "                            callbacks=[best_weights_checkpoint_callback_0, last_epoch_checkpoint_callback_0, csv_logger_callback_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6c57a-f91a-42d0-ac8f-b72a2a15f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved the history, so going to load in the history from the csv it was saved to\n",
    "history_0_loaded = pd.read_csv(f'logs/csv/{csv_logger_name_0}', index_col='epoch')\n",
    "history_0_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e07695-5ece-43f1-92ba-ec3612978694",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_performance = model_0.evaluate(test_data)\n",
    "model_0_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efdee57-80a5-4db1-a3bb-776f4348e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot.plot_history(history_0_loaded, 'loss')\n",
    "utils.plot.plot_history(history_0_loaded, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d439b53-8401-4fb6-8b4d-cf1c241ed4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Save model\n",
    "model_0.save(model_0_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d687a0-7775-43e9-bdf2-57da9d2b740d",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "After looking at the accuracy and loss curves above, looks to be that the training data is overfitting. Going to attempt to reduce overfitting in the next models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca6b46-35ed-49e5-9918-45022811bbce",
   "metadata": {},
   "source": [
    "## Fine Tuned Model\n",
    "\n",
    "To potentially reduce overfitting, I am going  to unfreeze the last 5 layers in the base model and retrain the model. Before I do this, I am going to update the checkpoint criteria due to the time it takes to train each model, that way I can run a few epochs at a time. The correction I made was I setup three total callbacks:\n",
    "\n",
    "1. Save checkpoint of the best fit weights\n",
    "2. Save checkpoint of the last epoch weights\n",
    "3. Save a CSV logger file to store the history of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e2881-266e-4e2e-bf4b-73a56862483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_path = 'models/101_food_classes/model_1'\n",
    "\n",
    "try:\n",
    "    model_1 = tf.keras.models.load_model(model_1_path)\n",
    "    train_model_1: bool = False\n",
    "except OSError:\n",
    "    train_model_1: bool = True\n",
    "\n",
    "print('Train Model 1: ', train_model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c528f8-409d-471f-969a-4200f6dbff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Callbacks\n",
    "best_weights_checkpoint_path_1 = 'checkpoints/101_classes_10_percent/model_1/best_epoch/checkpoint.ckpt'\n",
    "best_weights_checkpoint_callback_1 = generate_checkpoint_callback(best_weights_checkpoint_path_1)\n",
    "\n",
    "last_epoch_checkpoint_path_1 = 'checkpoints/101_classes_10_percent/model_1/last_epoch/checkpoint.ckpt'\n",
    "last_epoch_checkpoint_callback_1 = generate_checkpoint_callback(last_epoch_checkpoint_path_1, best_only=False)\n",
    "\n",
    "csv_logger_name_1 = '101_classes_10_percent/model_1.csv'\n",
    "csv_logger_callback_1 = generate_csv_logger_callback(csv_logger_name_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb6b81a-00f7-4aa3-b3bb-2349b9b2a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3. Starting from model_0 (Reloading in where model_0 left off from)\n",
    "model_1 = tf.keras.models.clone_model(model_0)\n",
    "model_1.load_weights(last_epoch_checkpoint_path_0)\n",
    "\n",
    "# Setting the efficient net model to be trainable\n",
    "base_model_1 = model_1.layers[2]\n",
    "base_model_1.trainable = True\n",
    "\n",
    "# Setting all but last 5 layers in base_model to be not traininable (only last 5 layers are trainable)\n",
    "for layer in base_model_1.layers[:-5]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d063b-1951-460e-a82b-55b2164706b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the last 5 layers are trainable\n",
    "for layer in base_model_1.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea190ca3-ab8f-424e-9645-84706b696254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compile Model with lower learning rate (typically 10X what the default learning rate was)\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78b326-0f21-4afb-acf0-03c14c5841f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Fit the Model\n",
    "percent_of_validation_data = 0.15\n",
    "\n",
    "# The starting epoch\n",
    "num_epochs_1 = num_epochs_0 + 5\n",
    "\n",
    "if train_model_1:\n",
    "    model_1.fit(train_data,\n",
    "                epochs=num_epochs_1,\n",
    "                initial_epoch=num_epochs_0,\n",
    "                steps_per_epoch=len(train_data),\n",
    "                validation_data=test_data,\n",
    "                validation_steps=int(percent_of_validation_data * len(test_data)),\n",
    "                callbacks=[\n",
    "                    best_weights_checkpoint_callback_1,\n",
    "                    last_epoch_checkpoint_callback_1,\n",
    "                    csv_logger_callback_1,\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802fa614-6998-4e03-a2c1-ce58115e757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_performance = model_1.evaluate(test_data)\n",
    "model_1_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d792487-4cb0-4be7-a0b1-1da57d293259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved the history, so going to load in the history from the csv it was saved to\n",
    "history_1_loaded = pd.read_csv(f'logs/csv/{csv_logger_name_1}', index_col='epoch')\n",
    "history_1_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e56659-bad3-4262-a3f9-63e74950933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot.plot_history(history_1_loaded, 'loss')\n",
    "utils.plot.plot_history(history_1_loaded, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a156a22-0d6e-4038-a676-cc868ecab266",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_histories(history_0_loaded, history_1_loaded, num_epochs_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbe881-c65b-458d-be43-936b1564d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Save model\n",
    "model_1.save(model_1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49c428-29cd-497b-9dc3-7144023b7d3c",
   "metadata": {},
   "source": [
    "## Evaluating Fine Tuned Model\n",
    "\n",
    "Let's make some predictions, visualize them, then find out which predictions were the most wrong. To do this, I am going to use the already fine tuned and exported model associated with the course this notebook follows at the link below:\n",
    "\n",
    "* https://storage.googleapis.com/ztm_tf_course/food_vision/06_101_food_class_10_percent_saved_big_dog_model.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943862eb-37bb-42b9-b0c1-dd3c938b6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_model_path = 'models/101_food_classes/06_101_food_class_10_percent_saved_big_dog_model'\n",
    "external_model = tf.keras.models.load_model(external_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ae04d-30db-4745-9151-0d28cb701359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EValuate the loaded model on the test data\n",
    "external_model_performance = external_model.evaluate(test_data)\n",
    "external_model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dff226-ddd3-4efb-bbb0-b917bfd3ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions with the model\n",
    "pred_probs = external_model.predict(test_data, verbose=1)  # Set verbose to see how long is left\n",
    "pred_probs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afdd937-d50d-4933-9547-2f65c2e59092",
   "metadata": {},
   "source": [
    "**NOTE**: Out model outpouts a prediction probability array for each image predicted in the above code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9e772-fe04-4615-9852-688e6a07020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at a sample of the predictions for the first image (a prediction probability array)\n",
    "pred_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35019e62-6e8d-4d41-979a-4fb5c12a38d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class index with the highest probability of our sample\n",
    "print(f'The class with the highest value for first image: {test_data.class_names[pred_probs[0].argmax()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e645a-596d-4e49-beae-4fddbeb72c15",
   "metadata": {},
   "source": [
    "### Comparing Predictions to Actual Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151a899-5347-4b1f-8d7d-851bacfd85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted class Indices\n",
    "pred_class_indices = pred_probs.argmax(axis=1)\n",
    "pred_class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9c771-ef8a-431f-ad56-382d9ae6c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Need to unbatch the test_data dataset\n",
    "y_labels = []\n",
    "\n",
    "for images, labels in test_data.unbatch():\n",
    "    # Currently, test labels look like [0,0,0,....1,0,...0,0,0] for each label due to one hot encoding\n",
    "    y_labels.append(labels.numpy().argmax())\n",
    "\n",
    "len(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba407e95-a582-4d1f-a684-02a5e5ce8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The labels with the test data are not shuffled, so the labels are in order\n",
    "y_labels[:10], y_labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1eaf25-682b-40f8-9b7c-720a659cf5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the accuracy score using sklearns accuracy score function\n",
    "accuracy_score(y_labels, pred_class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc9e2b-bb99-4379-abc3-2ce0e036e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a confusion matrix for our model using sklearns confusion_matrix function\n",
    "utils.plot.plot_confusion_matrix(y_labels, pred_class_indices, classes=test_data.class_names, figsize=(120,120))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5b013-9a84-41ff-9da3-c241c03b1e7b",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "After looking at the confusion matrix, the model performed decently well, however, foods that appear to look similar confuse the model. For instance, tira misue and chocolate cake are commonly mixed up, as well as springs rolls and samosas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d53ee5-351d-4e75-902c-caf3801b0787",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "\n",
    "Scitkit learn has a helpful function to get many classification metrics per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ee1fc-5544-46b8-9add-2e8176687712",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_labels, pred_class_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83a351-cd43-414d-a932-34aab8153476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to functionize this\n",
    "# Lets plot the classification report\n",
    "# Get the dictionary of the classification report\n",
    "model_classification_report = classification_report(y_labels, pred_class_indices, output_dict=True)\n",
    "\n",
    "# Get the f1 score metric and the corresponding class name\n",
    "class_name_to_f1_score = {}\n",
    "for class_number, metrics in model_classification_report.items():\n",
    "    # Multiple non-numeric keys occur which we don't want to store\n",
    "    try:\n",
    "        class_number = int(class_number)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    class_name=test_data.class_names[class_number]\n",
    "    class_name_to_f1_score[class_name] = metrics['f1-score']\n",
    "\n",
    "# Turn to a dataframe\n",
    "class_name_to_f1_score_df = pd.DataFrame({\n",
    "    'class_name': class_name_to_f1_score.keys(),\n",
    "    'f1_score': class_name_to_f1_score.values()\n",
    "})\n",
    "\n",
    "# Sort the dataframe\n",
    "class_name_to_f1_score_df = class_name_to_f1_score_df.sort_values('f1_score', ascending=True)\n",
    "\n",
    "# Plotting the data\n",
    "fig, ax = plt.subplots(figsize=(12,25))\n",
    "scores = ax.barh(range(len(class_name_to_f1_score_df)), class_name_to_f1_score_df['f1_score'].values)\n",
    "ax.set_yticks(range(len(class_name_to_f1_score_df)))\n",
    "ax.set_yticklabels(class_name_to_f1_score_df['class_name']);\n",
    "ax.set_xlabel('F1 Score')\n",
    "ax.set_title('F1 Score for 101 Food Classes (Predicted by Food Vision Mini)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e051c97a-c926-43ae-ac4e-18dd75b461db",
   "metadata": {},
   "source": [
    "### Visualizing Predictions on Custom Images\n",
    "\n",
    "To visualize our model's prediction on our own images, we'll need a function to load and preprocess images, specifically it will need to:\n",
    "\n",
    "* Read in a target image filepath using tf.io.read_file()\n",
    "* Turn the image into a tensor using tf.io.decode_image()\n",
    "* Resize the image tensor to be the same size as the images our model has trained on using tf.image.resize()\n",
    "* Scale the image to get all of the pixel values between 0 & 1 (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7dfbb-5128-4c88-bb47-19b5cd7a63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prep_image(filename: str, image_size: int = 224, scale: bool = True) -> tf.Tensor:\n",
    "    # Read in the image\n",
    "    image = tf.io.read_file(filename)\n",
    "\n",
    "    # Decode image into Tensor\n",
    "    image = tf.io.decode_image(image, channels=3)\n",
    "\n",
    "    # Resize image\n",
    "    image = tf.image.resize(image, [image_size, image_size])\n",
    "\n",
    "    # Scale Image to get all between 0 & 1 (not always required)\n",
    "    if scale:\n",
    "        image = image / 255.\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a02a5b-d9e9-4849-aa20-6efacdca37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying random image from test dataset, the predicted model, and our model's prediction\n",
    "plt.figure(figsize=(17,10))\n",
    "\n",
    "for i in range(3):\n",
    "    # Choose a random test image from a random class directory\n",
    "    random_class_name = random.choice(test_data.class_names)\n",
    "    test_class_dir = f'{str(test_directory)}/{random_class_name}'\n",
    "    filename = random.choice(os.listdir(f'{str(test_directory)}/{random_class_name}'))\n",
    "    file_path = f'{test_class_dir}/{filename}'\n",
    "\n",
    "    # Predict image\n",
    "    img = load_and_prep_image(file_path, scale=False)\n",
    "    expanded_img = tf.expand_dims(img, axis=0)\n",
    "    pred_prob = external_model.predict(expanded_img, verbose=0)\n",
    "    pred_class = test_data.class_names[pred_prob.argmax()]\n",
    "\n",
    "    # Plot images\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.imshow(img/255.)\n",
    "    if random_class_name == pred_class:\n",
    "        title_color = 'g'\n",
    "    else:\n",
    "        title_color = 'r'\n",
    "    plt.title(f'Actual: {random_class_name} | Predicted: {pred_class} | Prob: {pred_prob.max():.2f}', color=title_color, size=8)\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2872ab-8be5-483a-b6fd-940b3ca1dc47",
   "metadata": {},
   "source": [
    "### Identifying Most Incorrect Predictions\n",
    "\n",
    "To get a better insight on what is confusing the model, we are going to look at predictions that had the highest confidence in their prediction, but they ended up being wrong. This can reveal some of the following insights:\n",
    "\n",
    "* Data issues (wrong labels, e.g. model is correct but the true label is wrong)\n",
    "* Confusing classes (get better/more diverse data)\n",
    "\n",
    "To find out where the model is guessing wrong, going to find the following:\n",
    "\n",
    "1. Get all the image file paths in the test dataset\n",
    "2. Create a Pandas DF of the image file path, the true label, predicted class, and max predicted probabilities\n",
    "3. Use DF to find all the incorrect predictions\n",
    "4. Sort DF based on incorrect predictions with the highest probability predictions at the top\n",
    "5. Visualize the images with the most wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6906bff7-1b43-4a20-ac36-219a2f357b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. file paths in the test dataset\n",
    "test_data_file_paths = []\n",
    "for filepath in test_data.list_files(f'{str(test_directory)}/*/*.jpg', shuffle=False):\n",
    "    test_data_file_paths.append(filepath.numpy())\n",
    "len(test_data_file_paths), test_data_file_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb51060-7467-41b7-8ad0-1a03903acb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create pandas dataframe\n",
    "predicted_df = pd.DataFrame({\n",
    "    'filepath': test_data_file_paths,\n",
    "    'y_true': y_labels,\n",
    "    'predicted_class': pred_class_indices,\n",
    "    'predicted_confidence': pred_probs.max(axis=1),\n",
    "    'y_true_classname': [test_data.class_names[i] for i in y_labels],\n",
    "    'y_pred_classname': [test_data.class_names[i] for i in pred_class_indices],\n",
    "})\n",
    "predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935c267-2371-4cef-bf5b-fc1cb8a49c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use df to find incorrect predictions\n",
    "predicted_df['prediction_correct'] = predicted_df['y_true'] == predicted_df['predicted_class']\n",
    "predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129fb04-4348-4274-9828-e3d6067d0a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_incorrect_predicted_df = predicted_df[predicted_df['prediction_correct'] == False].sort_values('predicted_confidence', ascending=False)[:100]\n",
    "top_100_incorrect_predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc520337-b273-4d27-8771-68f218cb8c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualize test data samples with highest prediction that ended up being incorrect\n",
    "top_100_incorrect_predicted_df[:9]\n",
    "for row in top_100_incorrect_predicted_df[:9].iterrows():\n",
    "    print(row[1].filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c548983-d8ab-41e5-a63f-a64c299c33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying first 9 images from test dataset, the predicted model, and our model's prediction\n",
    "start_index = 0\n",
    "images_to_view = 9\n",
    "column = 0\n",
    "for i, row in enumerate(top_100_incorrect_predicted_df[start_index:start_index + images_to_view].iterrows()):\n",
    "    if i % 3 == 0:\n",
    "        plt.figure(figsize=(15,10))\n",
    "        column += 1\n",
    "    # Predict image\n",
    "    img = load_and_prep_image(row[1].filepath, scale=False)\n",
    "    expanded_img = tf.expand_dims(img, axis=0)\n",
    "\n",
    "    # Plot images    \n",
    "    plt.subplot(1, 3, i % 3 + 1)\n",
    "    plt.imshow(img/255.)\n",
    "    if row[1].y_true_classname == row[1].y_pred_classname:\n",
    "        title_color = 'g'\n",
    "    else:\n",
    "        title_color = 'r'\n",
    "    plt.title(f'Actual: {row[1].y_true_classname} | Predicted: {row[1].y_pred_classname} | Prob: {row[1].predicted_confidence:.2f}', color=title_color, size=8)\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328dcdc4-e434-4288-b321-f6cfd9b54e32",
   "metadata": {},
   "source": [
    "### Testing Model on Custom Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d99fd0-3a81-460d-b243-718b8e443812",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'data/food-101/model_testing/food'\n",
    "custom_image_filepaths = [f'{image_dir}/{filename}' for filename in os.listdir(image_dir)]\n",
    "\n",
    "for image_filename in custom_image_filepaths:\n",
    "    # Predict image\n",
    "    img = load_and_prep_image(image_filename, scale=False)\n",
    "    expanded_img = tf.expand_dims(img, axis=0)\n",
    "    pred_prob = external_model.predict(expanded_img, verbose=0)\n",
    "    pred_class = test_data.class_names[pred_prob.argmax()]\n",
    "\n",
    "    # Plot images\n",
    "    plt.figure()\n",
    "    plt.imshow(img/255.)\n",
    "    plt.title(f'Predicted: {pred_class} | Prob: {pred_prob.max():.2f}', size=8)\n",
    "    plt.axis(False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
