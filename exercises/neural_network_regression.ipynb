{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c40ac04",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks with TensorFlow\n",
    "\n",
    "We are going to simplify regression problems by predicting a numerical variable based on some other combination of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c08038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Plotting the model\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some sample data for regression\n",
    "X = tf.constant(np.array([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0]))\n",
    "y = tf.constant(np.array([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0]))  # y = x + 10\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af60558e",
   "metadata": {},
   "source": [
    "## Input and Output Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69238b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tensor for house info\n",
    "house_info = tf.constant(['bedroom', 'bathroom', 'garage'])  # Input\n",
    "house_price = tf.constant([939700])  # Output\n",
    "\n",
    "house_info, house_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d51603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuring out input and output shape for X and y?\n",
    "# NOTE: This doesn't work, the input shape should be (1,) and output shape should be (1,)\n",
    "input_shape = X.shape\n",
    "output_shape = y.shape\n",
    "input_shape, output_shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a0f6e77",
   "metadata": {},
   "source": [
    "## Steps in Modeling with TensorFlow\n",
    "\n",
    "1. Creating a model: define the input and output layers, as well as the hidden layers of a deep learning model.\n",
    "2. Compiling a model: define the loss function (in other words, the function which tellls our model how wrong it is) and the optimizer (tells our model how to improve the patterns it's learning) and evaluation metrics (what we can use to interpret the performance of our model).\n",
    "3. Fitting a model: letting the model try to find patterns between X and y (features and labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278455dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seet\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72c589f0",
   "metadata": {},
   "source": [
    "### Trial-1\n",
    "Creating a simple model to iterate and improve upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b6ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Create a model using sequential api\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(1)  # This defines the output layer of shape 1\n",
    "])\n",
    "model.input_shape, model.output_shape, model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ccee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compiling the model\n",
    "# losses.mae is Mean Absolute Error\n",
    "# In short, this takes the average of the error between the prediction and the actual value\n",
    "loss_function = tf.keras.losses.mae\n",
    "\n",
    "# optimizers.SGD is Stochastic Gradient Descent\n",
    "# NOTE: Using optimizers.legacy.SGD due to optimizers.SGD being slower on M1 Mac\n",
    "optimizer = tf.keras.optimizers.legacy.SGD()\n",
    "\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f794da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fit the model\n",
    "model.fit(X, y, epochs=5)  # epochs is how many times the model runs through the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbbdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing predictions of the model\n",
    "model.predict([17.0])  # This is pretty far off, so next steps are to improve the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fc742c6",
   "metadata": {},
   "source": [
    "### Improving the model\n",
    "\n",
    "We can improve our model by altering the steps we took to create a model.\n",
    "\n",
    "1. **Creating a Model**: We can add more layers, increase number of neurons in each hidden layer, change the activation function of each layer.\n",
    "2. **Compiling a Model**: We can change the loss function, use a different optimization function, or the learning rate of the optimization function.\n",
    "3. **Fitting a Model**: We can fit a model with more epochs (cycle through training data more times) or on more data (give more examples to learn from).\n",
    "\n",
    "**NOTE**: It is better to start with a smaller model, then impr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15e6963c",
   "metadata": {},
   "source": [
    "### Trial-2\n",
    "Only difference between Trial 1 and Trial 2 is I will increase the epochs (number of cycles trained through the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuilding/Improving the Model\n",
    "\n",
    "# 1. Create a model using sequential api\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(1)  # This defines the output layer of shape 1\n",
    "])\n",
    "\n",
    "# 2. Compiling the model\n",
    "# losses.mae is Mean Absolute Error\n",
    "# In short, this takes the average of the error between the prediction and the actual value\n",
    "loss_function = tf.keras.losses.mae\n",
    "\n",
    "# optimizers.SGD is Stochastic Gradient Descent\n",
    "# NOTE: Using optimizers.legacy.SGD due to optimizers.SGD being slower on M1 Mac\n",
    "optimizer = tf.keras.optimizers.legacy.SGD()\n",
    "\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae'])\n",
    "\n",
    "# 3. Fit the model (training for 100 epochs instead of 5)\n",
    "model.fit(X, y, epochs=100)  # epochs is how many times the model runs through the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96871961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing predictions of the model\n",
    "model.predict([17.0])  # This is pretty far off, so next steps are to improve the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fbc1cce",
   "metadata": {},
   "source": [
    "### Trial-3\n",
    "Keeping epochs the same as Trial-2, but adding a hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f943a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuilding/Improving the Model\n",
    "\n",
    "# 1. Create a model using sequential api\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(100, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compiling the Model\n",
    "loss_function = tf.keras.losses.mae\n",
    "optimizer = tf.keras.optimizers.legacy.SGD()\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "# 3. Fit the Model\n",
    "model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c36f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing predictions of the model (Actual value should be 27)\n",
    "model.predict([17.0])  # Weird, this value prediction is not even close to the value, even though the mae is lower"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82730e92",
   "metadata": {},
   "source": [
    "### Trial-4\n",
    "Keeping epochs the same as Trial-2 and hidden layers from Trial-3, but changing the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ff122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuilding/Improving the Model\n",
    "\n",
    "# 1. Create a model using sequential api\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,)),\n",
    "    tf.keras.layers.Dense(100, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compiling the Model\n",
    "loss_function = tf.keras.losses.mae\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(lr=0.01)\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=['mae'])\n",
    "\n",
    "# 3. Fit the Model\n",
    "model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c6d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing predictions of the model (Actual value should be 27)\n",
    "model.predict([17.0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da88ade8",
   "metadata": {},
   "source": [
    "## Evaluating a Model\n",
    "\n",
    "In practive, a typical workflow you'll go through consists of the following:\n",
    "\n",
    "create a model -> fit it -> evaluat it  -> tweak it -> repeat....\n",
    "\n",
    "**Most Important to Eavluating**\n",
    "Most important step of eavluating a model is to Visualize. It's a good idea to visualize:\n",
    "* The data: what data are we working with? What does it look like?\n",
    "* The model: What does our model look like?\n",
    "* The training of the model: How does aa model perform while it learns?\n",
    "* The predictions of the model: How accurate are the predictions?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0bca3c7",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "What is a good way to train a model? Give it a bigger dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a bigger set of data.\n",
    "X = tf.range(-100, 100, 4)\n",
    "y = X + 10\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbb9d193",
   "metadata": {},
   "source": [
    "### Splitting Data\n",
    "Need to split my complete data set into a training set and a test set\n",
    "\n",
    "**NOTE: There are typically 3 sets to split the data with**\n",
    "1. **Training Set**: Typically 70-80% of the data available used to train the model\n",
    "2. **Validation Set**: The data the model gets tuned on. Typically 10-15%.\n",
    "3. **Test Set**: The model gets evaluated on this data to teest what it has learned. Typically 10-15% of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2aa9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figuring out the Training and Test Set Sizes\n",
    "total_units = len(X)\n",
    "total_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abbb5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training set (80%)\n",
    "# NOTE: Typically, we should shuffle the data before splitting\n",
    "\n",
    "X_train = X[:int(total_units * 0.8)]\n",
    "y_train = y[:int(total_units * 0.8)]\n",
    "\n",
    "X_test = X[int(total_units * 0.8):]\n",
    "y_test = y[int(total_units * 0.8):]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the data\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(X_train, y_train, c='b', label='Training Data')\n",
    "plt.scatter(X_test, y_test, c='g', label='Test Data')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e3dc2c8",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d24808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,), name='InputLayer'),\n",
    "    tf.keras.layers.Dense(100, activation=tf.keras.activations.relu, name='HiddenLayer-1'),\n",
    "    tf.keras.layers.Dense(1, name='OutputLayer'),\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "model.compile(loss=tf.keras.losses.mae,\n",
    "              optimizer=tf.keras.optimizers.legacy.Adam(lr=0.005),\n",
    "             metrics=['mae'])\n",
    "\n",
    "\n",
    "# 3. Fit Model\n",
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11076386",
   "metadata": {},
   "source": [
    "### Visualizing the model\n",
    "\n",
    "Visualizing the model can be done with a handful of tools.\n",
    "\n",
    "* Summary: Looking at the summary of the model and what it looks like.\n",
    "* Diagram: Looking at the diagram of the model.\n",
    "* Plot: Plotting the predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee079ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06956373",
   "metadata": {},
   "source": [
    "* Total params: the total parameters in the model.\n",
    "* Trainable params: the parameters (patterns) the model can update as it trains.\n",
    "* Non-trainable params: the parameters (patterns) that the model does not update as it trains. This occurrs when using an already trained model from transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e7fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc676dca",
   "metadata": {},
   "source": [
    "#### Visualizing the Predictions\n",
    "\n",
    "Plotting predictions against the actual values can help better visualize the model. This is often in the form of y_test or y_true versus y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Some predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c98e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a plotting function\n",
    "def plot_predictions(train_data=X_train, \n",
    "                     train_labels=y_train,\n",
    "                     test_data=X_test,\n",
    "                     test_labels=y_test,\n",
    "                     predictions=y_pred):\n",
    "    \"\"\"\n",
    "    Plots training data, test data, and compares predictions against actual valus.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(train_data, train_labels, c='b', label='Training Data')\n",
    "    plt.scatter(test_data, test_labels, c='g', label='Test Data')\n",
    "    plt.scatter(test_data, predictions, c='r', label='Predictions')\n",
    "    plt.legend()\n",
    "\n",
    "plot_predictions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fd89526",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "Depending on the problem, there will be different evaluation metrics to evaluate your model's performance.\n",
    "\n",
    "Common Regression Metrics:\n",
    "* **Mean Absolute Error**: The average absolute value of the difference between the prediction and the actual value.\n",
    "* **Mean Square Error**: The average of the square of the difference between the prediction and the actual value.\n",
    "  - Note: This is useful because the larger the difference, the MSE is drastically larger than just looking at the difference between the prediction and the actual value. TLDR: larger erros are more significant than smaller errors.\n",
    "* **Huber**: Combination of MSE and MAE. Less sensitive to outliers than MSE, but more sensitive to larger errors than MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f03150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = tf.metrics.mean_absolute_error(y_true=tf.squeeze(y_test), y_pred=tf.squeeze(y_pred))\n",
    "mse = tf.metrics.mean_squared_error(y_true=tf.squeeze(y_test), y_pred=tf.squeeze(y_pred))\n",
    "mae, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc13483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, pred):\n",
    "    return tf.metrics.mean_absolute_error(y_true=tf.squeeze(y_true), y_pred=tf.squeeze(pred))\n",
    "\n",
    "def mse(y_true, pred):\n",
    "    return tf.metrics.mean_squared_error(y_true=tf.squeeze(y_true), y_pred=tf.squeeze(pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ab2df93",
   "metadata": {},
   "source": [
    "### Resetting and Iterating My Model to Optimize It\n",
    "\n",
    "Going to test out 3 experiments with the same data as before.\n",
    "\n",
    "1. Trial-1: Simple Model\n",
    "2. Trial-2: Increase epochs to 100\n",
    "3. Trial-3: Add a 2nd Hidden Layer with epochs at 100\n",
    "4. Trial-4: Add a 2nd Hidden Layer with epochs at 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6adc796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting up the data and setup\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "X = tf.range(-100, 100, 4)\n",
    "y = X + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7367de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Data\n",
    "num_training_points = int(len(X) * 0.8)\n",
    "\n",
    "X_train = X[:num_training_points]\n",
    "y_train = y[:num_training_points]\n",
    "\n",
    "X_test = X[num_training_points:]\n",
    "y_test = y[num_training_points:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "663b0daf",
   "metadata": {},
   "source": [
    "#### Trial-1: Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d49f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Model\n",
    "model_1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,), name='InputLayer'),\n",
    "    tf.keras.layers.Dense(50, activation=tf.keras.activations.relu, name='HiddenLayer-1'),\n",
    "    tf.keras.layers.Dense(1, name='OutputLayer')\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "model_1.compile(optimizer=tf.keras.optimizers.legacy.Adam(lr=0.005),\n",
    "              loss=tf.keras.losses.mae)\n",
    "\n",
    "# 3. Fit Model\n",
    "model_1.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065e188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Model\n",
    "predictions_1 = model_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2928ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model\n",
    "plot_predictions(train_data=X_train, \n",
    "                 train_labels=y_train,\n",
    "                 test_data=X_test,\n",
    "                 test_labels=y_test,\n",
    "                 predictions=predictions_1)\n",
    "\n",
    "mae_1 = mae(y_test, predictions_1 )\n",
    "mse_1 = mse(y_test, predictions_1 )\n",
    "mae_1, mse_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86712ce2",
   "metadata": {},
   "source": [
    "#### Trial-2: Epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f01545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Model\n",
    "model_2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,), name='InputLayer'),\n",
    "    tf.keras.layers.Dense(50, activation=tf.keras.activations.relu, name='HiddenLayer-1'),\n",
    "    tf.keras.layers.Dense(1, name='OutputLayer')\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "model_2.compile(optimizer=tf.keras.optimizers.legacy.Adam(lr=0.005),\n",
    "              loss=tf.keras.losses.mae)\n",
    "\n",
    "# 3. Fit Model\n",
    "model_2.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Model\n",
    "predictions_2 = model_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model\n",
    "plot_predictions(train_data=X_train, \n",
    "                 train_labels=y_train,\n",
    "                 test_data=X_test,\n",
    "                 test_labels=y_test,\n",
    "                 predictions=predictions_2)\n",
    "\n",
    "mae_2 = mae(y_test, predictions_2 )\n",
    "mse_2 = mse(y_test, predictions_2 )\n",
    "mae_2, mse_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2681ead",
   "metadata": {},
   "source": [
    "#### Trial-3: Epochs-100, Hidden Layers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e47bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Model\n",
    "model_3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,), name='InputLayer'),\n",
    "    tf.keras.layers.Dense(50, activation=tf.keras.activations.relu, name='HiddenLayer-1'),\n",
    "    tf.keras.layers.Dense(50, activation=tf.keras.activations.relu, name='HiddenLayer-2'),\n",
    "    tf.keras.layers.Dense(1, name='OutputLayer')\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "model_3.compile(optimizer=tf.keras.optimizers.legacy.Adam(lr=0.005),\n",
    "              loss=tf.keras.losses.mae)\n",
    "\n",
    "# 3. Fit Model\n",
    "model_3.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01647180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Model\n",
    "predictions_3 = model_3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model\n",
    "plot_predictions(train_data=X_train, \n",
    "                 train_labels=y_train,\n",
    "                 test_data=X_test,\n",
    "                 test_labels=y_test,\n",
    "                 predictions=predictions_3)\n",
    "\n",
    "mae_3 = mae(y_test, predictions_3 )\n",
    "mse_3 = mse(y_test, predictions_3 )\n",
    "mae_3, mse_3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99da2bde",
   "metadata": {},
   "source": [
    "#### Trial 4: Epochs=500, Hidden Layers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Model\n",
    "model_4 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1,), name='InputLayer'),\n",
    "    tf.keras.layers.Dense(50, activation=tf.keras.activations.relu, name='HiddenLayer-1'),\n",
    "    tf.keras.layers.Dense(50, activation=tf.keras.activations.relu, name='HiddenLayer-2'),\n",
    "    tf.keras.layers.Dense(1, name='OutputLayer')\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "model_4.compile(optimizer=tf.keras.optimizers.legacy.Adam(lr=0.005),\n",
    "              loss=tf.keras.losses.mae)\n",
    "\n",
    "# 3. Fit Model\n",
    "model_4.fit(X_train, y_train, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c820ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Model\n",
    "predictions_4 = model_4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b593e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model\n",
    "plot_predictions(train_data=X_train, \n",
    "                 train_labels=y_train,\n",
    "                 test_data=X_test,\n",
    "                 test_labels=y_test,\n",
    "                 predictions=predictions_4 )\n",
    "\n",
    "mae_4 = mae(y_test, predictions_4 )\n",
    "mse_4 = mse(y_test, predictions_4 )\n",
    "mae_4, mse_4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9beafb3",
   "metadata": {},
   "source": [
    "#### Comparing Each Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = [['model_1', mae_1.numpy(), mse_1.numpy()],\n",
    "                 ['model_2', mae_2.numpy(), mse_2.numpy()],\n",
    "                 ['model_3', mae_3.numpy(), mse_3.numpy()],\n",
    "                 ['model_4', mae_4.numpy(), mse_4.numpy()]]\n",
    "\n",
    "all_results = pd.DataFrame(model_results, columns=['model', 'mae', 'mse'])\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad00a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_3, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b00ce96",
   "metadata": {},
   "source": [
    "## Tracking Experiments\n",
    "\n",
    "A good habit for machine learning is to track experiments and their corresponding results.\n",
    "\n",
    "Useful tools to help track experiments:\n",
    "* TensorBoard: Component of the TensorFlow library to help track modeling experiments.\n",
    "* Weights & Biases: tool used for tracking experiments (Should def look this one up!)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0a00663",
   "metadata": {},
   "source": [
    "## Saving Our Models\n",
    "\n",
    "Saving out models allows us to use them outside of the place they were trained. \n",
    "\n",
    "* TensorFlow Docs: https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "* Saving model can be done by `model.save(\"savedirpath\")` or `model.save(\"h5filepath.h5\")`\n",
    "* Loading model can be done by `tf.keras.models.load_model(\"savedirpath\")` or `tf.keras.models.load_model(\"h5filepath.h5\")`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea2dff7f",
   "metadata": {},
   "source": [
    "# Testing Models w/ TensorFlow Using Medical Insurance Data\n",
    "\n",
    "I want to get some experience seeing different functionality TensorFlow offers, so going to use a general dataset to do so.\n",
    "\n",
    "Going to look at the medical cost dataset from Kaggle.\n",
    "* https://www.kaggle.com/datasets/mirichoi0218/insurance?r"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d52f4cf",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fc5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from src import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8680345",
   "metadata": {},
   "source": [
    "## Step-0: Get & Analyze the Data\n",
    "\n",
    "Need to look at the data to see what needs to be done to build out a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset from the raw csv file on the public github file\n",
    "csv_dataset_url = 'https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv'\n",
    "insurance = pd.read_csv(csv_dataset_url)\n",
    "insurance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d259e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the unique values in the age column\n",
    "np.unique(insurance['age'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the unique values in the children column\n",
    "np.unique(insurance['children'].to_numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7435f13d",
   "metadata": {},
   "source": [
    "## Step-1: Pre-Process Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4b4db3d",
   "metadata": {},
   "source": [
    "### Normalize & Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Feature Scaler and One Hot Encoder\n",
    "column_transformer = make_column_transformer(\n",
    "    (MinMaxScaler(), ['age', 'bmi', 'children']),  # normalize all value in these columns between 0 and 1\n",
    "    (OneHotEncoder(handle_unknown='ignore'), ['sex', 'smoker', 'region']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db90840a",
   "metadata": {},
   "source": [
    "## Step-2: Split Test & Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd717db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating out the X and y\n",
    "y_column_name = 'charges'\n",
    "X_df = insurance.drop(y_column_name, axis=1)\n",
    "y_df = insurance[y_column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data to test and train\n",
    "# NOTE: We do not need to convert these to tensors, Pandas is built on top of numpy which is handled directly\n",
    "# with the model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the column transformer to our training data\n",
    "column_transformer.fit(X_train)\n",
    "X_train_normal = column_transformer.transform(X_train)\n",
    "X_test_normal = column_transformer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f04d6dcf",
   "metadata": {},
   "source": [
    "### Viewing Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b3b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normal[0].shape, y_train[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed5a9668",
   "metadata": {},
   "source": [
    "## Step-3: Create, Compile, Fit, & Evaluate Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37c1fa34",
   "metadata": {},
   "source": [
    "### Model-1: Using a Simple Model \n",
    "1. Hidden Layer of 100\n",
    "2. SGD - learning rate = 0.01\n",
    "3. 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbaac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed to compare results\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create Model\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(11,)),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "learning_rate = 0.01\n",
    "model_1.compile(loss=tf.keras.losses.mae,\n",
    "                optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate),\n",
    "                metrics=['mae'])\n",
    "\n",
    "# 3. Fit Model\n",
    "history_1 = model_1.fit(X_train_normal, y_train, epochs=25, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f1598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate Model\n",
    "evaluate_1 = model_1.evaluate(X_test_normal, y_test)\n",
    "y_pred_1 = model_1.predict(X_test_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot.plot_history(history_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed222e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot.plot_true_versus_predicted(y_test, y_pred_1, figsize=(8, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "726f9752",
   "metadata": {},
   "source": [
    "### Model-2: Increasing Epochs\n",
    "1. Hidden Layer of 100\n",
    "2. SGD - learning rate = 0.01\n",
    "3. 75 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create Model\n",
    "model_2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(11,)),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "learning_rate = 0.01\n",
    "model_2.compile(loss=tf.keras.losses.mae,\n",
    "                optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=learning_rate),\n",
    "                metrics=['mae'])\n",
    "\n",
    "# 3. Fit Model\n",
    "history_2 = model_2.fit(X_train_normal, y_train, epochs=75, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e56da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate Model\n",
    "evaluate2 = model_2.evaluate(X_test_normal, y_test)\n",
    "y_pred_2 = model_2.predict(X_test_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad9744",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot.plot_history(history_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7181669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot.plot_true_versus_predicted(y_test, y_pred_2, figsize=(8, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03deb937",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "The epochs steadied out the loss right around 25 epochs, so increasing the epochs only made it slightly more accurate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a00f0c4d",
   "metadata": {},
   "source": [
    "### Model-3: \n",
    "1. Hidden Layer of 100\n",
    "2. Adam - learning_rate=0.01\n",
    "3. 75 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c149eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create Model\n",
    "model_3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(11,)),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "learning_rate = 0.01\n",
    "model_3.compile(loss=tf.keras.losses.mae,\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate),\n",
    "                metrics=['mae'])\n",
    "\n",
    "# 3. Fit Model\n",
    "history_3 = model_3.fit(X_train_normal, y_train, epochs=75, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb11477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate Model\n",
    "evaluate3 = model_3.evaluate(X_test_normal, y_test)\n",
    "y_pred_3 = model_3.predict(X_test_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b73149",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot.plot_history(history_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot.plot_true_versus_predicted(y_test, y_pred_3, figsize=(8, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "206a669d",
   "metadata": {},
   "source": [
    "## Looking at Learning Rates\n",
    "\n",
    "Now that I have a handful of Models to compare against, I want to start looking at different ways of updating the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdfbc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create Model\n",
    "model_4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(11,)),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "initial_learning_rate = 0.001\n",
    "model_4.compile(loss=tf.keras.losses.mae,\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=initial_learning_rate),\n",
    "                metrics=['mae'])\n",
    "\n",
    "lr_scheduler_callback = utils.learning_rate.exponential_decay_callback(initial_learning_rate, decay_factor=2)\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler_callback)\n",
    "\n",
    "\n",
    "# 3. Fit Model\n",
    "epochs = 90\n",
    "history_4 = model_4.fit(X_train_normal, y_train, epochs=epochs, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [lr_scheduler_callback(epoch) for epoch in range(epochs)]\n",
    "utils.plot.plot_learning_rate_versus_loss(learning_rates, history_4.history['loss'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ccf2b8b",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "From the above graph, it looks like the optimal learning rate is between .05-.08."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4065f8f7",
   "metadata": {},
   "source": [
    "### Working with Callbacks\n",
    "\n",
    "1. EarlyStopping\n",
    "2. LambdaCallback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5c24201",
   "metadata": {},
   "source": [
    "#### Testing out the EarlyStopping Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667fda2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out the EarlyStopping Epoch Callback\n",
    "\n",
    "# Setting seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create Model\n",
    "model_5 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(11,)),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "initial_learning_rate = 0.06\n",
    "model_5.compile(loss=tf.keras.losses.mae,\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=initial_learning_rate),\n",
    "                metrics=['mae'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=20,\n",
    "    start_from_epoch=10,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# 3. Fit Model\n",
    "epochs = 200\n",
    "history_5 = model_5.fit(X_train_normal, y_train, epochs=epochs, callbacks=[early_stopping])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d8e596c",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "\n",
    "The lowest mae epoch was epoch 45. With the patience set to 20 for the callback, the last epoch was 65, at which point no epoch value was lower than the lowest mae at epoch 45, at which point it finished training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "015e4485",
   "metadata": {},
   "source": [
    "#### Testing Lambda Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5be554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out the LambdaCallback Epoch Callback\n",
    "\n",
    "# Setting seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create Model\n",
    "model_6 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(11,)),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile Model\n",
    "initial_learning_rate = 0.06\n",
    "model_6.compile(loss=tf.keras.losses.mae,\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=initial_learning_rate),\n",
    "                metrics=['mae'])\n",
    "\n",
    "lambda_callback = tf.keras.callbacks.LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch, logs: print(f'Epoch Start: {epoch}'),\n",
    "    on_epoch_end=lambda epoch, logs: print(f'Epoch End: {epoch}', logs))\n",
    "\n",
    "# 3. Fit Model\n",
    "epochs = 10\n",
    "history_6 = model_6.fit(X_train_normal, y_train, epochs=epochs, callbacks=[lambda_callback], verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7dc2f7b",
   "metadata": {},
   "source": [
    "### Working with Optimizer Callbacks\n",
    "\n",
    "1. ExponentialDecay Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d6333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out Optimizer ExponentialDecay Learning Rate Scheduler Callbacks\n",
    "\n",
    "# Setting seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create Model\n",
    "model_7 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(11,)),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile Model w/ ExponentialDecay\n",
    "initial_learning_rate = 0.05\n",
    "decay_steps = 1000000\n",
    "decay_rate = 0.05\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate)\n",
    "\n",
    "model_7.compile(loss=tf.keras.losses.mae,\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=lr_schedule),\n",
    "                metrics=['mae'])\n",
    "\n",
    "# 3. Fit Model\n",
    "epochs = 40\n",
    "history_7 = model_7.fit(X_train_normal, y_train, epochs=epochs, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405509b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate Model\n",
    "evaluate7 = model_7.evaluate(X_test_normal, y_test)\n",
    "y_pred_7 = model_7.predict(X_test_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot.plot_history(history_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df215120",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler_callback_7 = utils.learning_rate.exponential_decay_callback(\n",
    "    initial_learning_rate, decay_factor=decay_rate, decay_step=decay_steps)\n",
    "learning_rates = [lr_scheduler_callback_7(epoch) for epoch in range(epochs)]\n",
    "utils.plot.plot_learning_rate_versus_loss(learning_rates, history_7.history['loss'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccf7790e",
   "metadata": {},
   "source": [
    "### Quickly Testing Different Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c984b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out Optimizer ExponentialDecay Learning Rate Scheduler Callbacks\n",
    "\n",
    "# Setting seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create Model\n",
    "model_8 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(11,)),\n",
    "    tf.keras.layers.Dense(200),\n",
    "    tf.keras.layers.Dense(200, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile Model w/ ExponentialDecay\n",
    "initial_learning_rate = 0.05\n",
    "decay_steps = 1000000\n",
    "decay_rate = 0.05\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate)\n",
    "\n",
    "model_8.compile(loss=tf.keras.losses.mae,\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, weight_decay=0.001),\n",
    "                metrics=['mae'])\n",
    "\n",
    "# 3. Fit Model\n",
    "epochs = 40\n",
    "history_8 = model_8.fit(\n",
    "    X_train_normal,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebb26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate Model\n",
    "evaluate8 = model_8.evaluate(X_test_normal, y_test)\n",
    "y_pred_8 = model_8.predict(X_test_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107bf4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot.plot_history(history_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d5ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out Optimizer CosineRestart Learning Rate Scheduler Callbacks\n",
    "\n",
    "# Setting seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create Model\n",
    "model_9 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(11,)),\n",
    "    tf.keras.layers.Dense(200),\n",
    "    tf.keras.layers.Dense(200, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile Model w/ ExponentialDecay\n",
    "initial_learning_rate = 0.05\n",
    "decay_steps = 1450\n",
    "decay_rate = 0.05\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=initial_learning_rate, decay_steps=decay_steps)\n",
    "\n",
    "model_9.compile(loss=tf.keras.losses.mae,\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, weight_decay=0.001),\n",
    "                metrics=['mae'])\n",
    "\n",
    "# 3. Fit Model\n",
    "epochs = 40\n",
    "history_9 = model_9.fit(\n",
    "    X_train_normal,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f48d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate Model\n",
    "evaluate9 = model_9.evaluate(X_test_normal, y_test)\n",
    "y_pred_9 = model_9.predict(X_test_normal)\n",
    "evaluate9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17eaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot.plot_history(history_9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1acefa12",
   "metadata": {},
   "source": [
    "### Figuring Out Good Step Size for CosineDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out Optimizer CosineDecay Learning Rate Scheduler Callbacks\n",
    "epochs = 40\n",
    "initial_learning_rate = 0.05\n",
    "\n",
    "decay_steps_values = np.arange(100, 10100, 100)\n",
    "losses_per_step_test = []\n",
    "\n",
    "for decay_steps in decay_steps_values: \n",
    "    # Setting seed\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    # 1. Create Model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(11,)),\n",
    "        tf.keras.layers.Dense(200),\n",
    "        tf.keras.layers.Dense(200, activation=tf.keras.activations.relu),\n",
    "        tf.keras.layers.Dense(100),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # 2. Compile Model w/ ExponentialDecay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=initial_learning_rate, decay_steps=decay_steps)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.mae,\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, weight_decay=0.001),\n",
    "                  metrics=['mae'])\n",
    "\n",
    "    # 3. Fit Model\n",
    "    model.fit(\n",
    "        X_train_normal,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        verbose=0)\n",
    "\n",
    "    # 4. Evaluate Model\n",
    "    evaluate = model.evaluate(X_test_normal, y_test)\n",
    "    losses_per_step_test.append(evaluate[0])  # loss is first value in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c69d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_per_epoch = np.array(losses_per_step_test)\n",
    "losses_per_epoch.argmin() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f5936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_per_epoch[14], decay_steps_values[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54bd074",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(decay_steps_values, losses_per_epoch)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10a7b3bb",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "A step size of 1500 for the cosine decay function seemed to work best."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3ecc35b",
   "metadata": {},
   "source": [
    "### Testing Batch Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390269b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "\n",
    "# Cosine Decay Parameters\n",
    "initial_learning_rate = 0.05\n",
    "decay_steps = 1500\n",
    "\n",
    "batch_size_tests = np.arange(5, 105, 5)\n",
    "batch_size_loss_values = []\n",
    "for batch_size in batch_size_tests: \n",
    "    # Setting seed\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    # 1. Create Model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(11,)),\n",
    "        tf.keras.layers.Dense(200),\n",
    "        tf.keras.layers.Dense(200, activation=tf.keras.activations.relu),\n",
    "        tf.keras.layers.Dense(100),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # 2. Compile Model w/ ExponentialDecay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=initial_learning_rate, decay_steps=decay_steps)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.mae,\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, weight_decay=0.001),\n",
    "                  metrics=['mae'])\n",
    "\n",
    "    # 3. Fit Model\n",
    "    model.fit(\n",
    "        X_train_normal,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0)\n",
    "\n",
    "    # 4. Evaluate Model\n",
    "    evaluate = model.evaluate(X_test_normal, y_test)\n",
    "    batch_size_loss_values.append(evaluate[0])  # loss is first value in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_per_epoch_batch_size = np.array(batch_size_loss_values)\n",
    "losses_per_epoch_batch_size.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_per_epoch_batch_size[5], batch_size_tests[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd46bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(batch_size_tests, losses_per_epoch_batch_size)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "effb2ada",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "Looks like a batch size of 30 was the optimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f83f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cosine Decay Parameters\n",
    "initial_learning_rate = 0.05\n",
    "decay_steps = 1500\n",
    "\n",
    "# Setting Batch size and epochs\n",
    "batch_size = 30\n",
    "epochs = 40\n",
    "\n",
    "\n",
    "# 1. Create Model\n",
    "model_10 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(11,)),\n",
    "    tf.keras.layers.Dense(200),\n",
    "    tf.keras.layers.Dense(200, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile Model w/ ExponentialDecay\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=initial_learning_rate, decay_steps=decay_steps)\n",
    "\n",
    "model_10.compile(loss=tf.keras.losses.mae,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, weight_decay=0.001),\n",
    "              metrics=['mae'])\n",
    "\n",
    "# 3. Fit Model\n",
    "model_10.fit(\n",
    "    X_train_normal,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=0)\n",
    "\n",
    "# 4. Evaluate Model\n",
    "evaluate = model_10.evaluate(X_test_normal, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_10 = model_best.predict(X_test_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b1f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot.plot_true_versus_predicted(y_test, y_pred_10, figsize=(8, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "716f55da",
   "metadata": {},
   "source": [
    "### Testing Different Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cosine Decay Parameters\n",
    "initial_learning_rate = 0.05\n",
    "decay_steps = 1500\n",
    "\n",
    "# Setting Batch size and epochs\n",
    "batch_size = 30\n",
    "epochs = 40\n",
    "\n",
    "\n",
    "# 1. Create Model\n",
    "model_11 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(11,)),\n",
    "    tf.keras.layers.Dense(200),\n",
    "    tf.keras.layers.Dense(200, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile Model w/ ExponentialDecay\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=initial_learning_rate, decay_steps=decay_steps)\n",
    "\n",
    "model_11.compile(loss=tf.keras.losses.huber,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, weight_decay=0.001),\n",
    "              metrics=['mae', 'mse'])\n",
    "\n",
    "# 3. Fit Model\n",
    "history_11 = model_11.fit(\n",
    "    X_train_normal,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=0)\n",
    "\n",
    "# 4. Evaluate Model\n",
    "evaluate = model_11.evaluate(X_test_normal, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65c0d0de",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "Tested MSE, MAE, and Huber. MAE and Huber outperformed MSE significantly, and Huber slightly outperformed MAE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
